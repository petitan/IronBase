<mxfile host="app.diagrams.net" modified="2025-11-12T00:00:00.000Z" agent="Claude Code Ultra-Detailed" version="24.0.0" etag="MongoLite-v2-ultra" type="device">
  <diagram name="MongoLite Ultra-Detailed Architecture" id="ultra-detailed">
    <mxGraphModel dx="3200" dy="2000" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="4800" pageHeight="3600" math="0" shadow="0">
      <root>
        <mxCell id="0" />
        <mxCell id="1" parent="0" />

        <!-- ==================== MAIN TITLE ==================== -->
        <mxCell id="title_main" value="MongoLite Architecture - ULTRA-DETAILED DOCUMENTATION" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1;fontColor=#000000;" vertex="1" parent="1">
          <mxGeometry x="800" y="20" width="3200" height="60" as="geometry" />
        </mxCell>

        <mxCell id="subtitle_main" value="Complete implementation with algorithms, memory layouts, byte-level formats, call stacks, and performance profiling" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=18;fontStyle=2;fontColor=#666666;" vertex="1" parent="1">
          <mxGeometry x="800" y="80" width="3200" height="40" as="geometry" />
        </mxCell>

        <!-- ==================== SECTION 1: MEMORY LAYOUT ==================== -->
        <mxCell id="memory_section" value="SECTION 1: MEMORY LAYOUT &amp; DATA STRUCTURES" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#000000;strokeColor=#000000;fontColor=#FFFFFF;verticalAlign=top;fontSize=20;fontStyle=1;align=center;" vertex="1" parent="1">
          <mxGeometry x="40" y="140" width="4720" height="800" as="geometry" />
        </mxCell>

        <!-- Header Struct Memory Layout -->
        <mxCell id="header_memory" value="Header Struct - Memory Layout (256 bytes total)" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="80" y="185" width="900" height="730" as="geometry" />
        </mxCell>

        <mxCell id="header_memory_detail" value="&#xa;Rust Definition:&#xa;#[derive(Serialize, Deserialize, Clone, Debug)]&#xa;#[repr(C)]  // C-compatible layout for bincode&#xa;pub struct Header {&#xa;    pub magic: [u8; 8],              // Offset 0x00, Size: 8 bytes&#xa;    pub version: u32,                // Offset 0x08, Size: 4 bytes&#xa;    pub page_size: u32,              // Offset 0x0C, Size: 4 bytes&#xa;    pub collection_count: u32,       // Offset 0x10, Size: 4 bytes&#xa;    pub free_list_head: u64,         // Offset 0x14, Size: 8 bytes&#xa;    pub index_section_offset: u64,   // Offset 0x1C, Size: 8 bytes&#xa;    #[serde(default)]&#xa;    pub metadata_offset: u64,        // Offset 0x24, Size: 8 bytes&#xa;    #[serde(default)]&#xa;    pub metadata_size: u64,          // Offset 0x2C, Size: 8 bytes&#xa;    // Padding to 256 bytes by bincode&#xa;}&#xa;&#xa;Memory Layout (Little-Endian x86_64):&#xa;┌──────────────────────────────────────────────────────────┐&#xa;│ Offset │ Field Name           │ Type  │ Size │ Value    │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x0000 │ magic                │ [u8;8]│  8B  │ MONGOLTE │&#xa;│        │   Hex: 4D 4F 4E 47 4F 4C 54 45               │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x0008 │ version              │ u32   │  4B  │ 2        │&#xa;│        │   Hex: 02 00 00 00 (little-endian)           │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x000C │ page_size            │ u32   │  4B  │ 4096     │&#xa;│        │   Hex: 00 10 00 00                           │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x0010 │ collection_count     │ u32   │  4B  │ Variable │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x0014 │ free_list_head       │ u64   │  8B  │ 0        │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x001C │ index_section_offset │ u64   │  8B  │ 0        │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x0024 │ metadata_offset      │ u64   │  8B  │ Dynamic  │&#xa;│        │   Example: 0x0001A4B0 (107,696 bytes)        │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x002C │ metadata_size        │ u64   │  8B  │ Variable │&#xa;│        │   Example: 0x00003A98 (15,000 bytes)         │&#xa;├──────────────────────────────────────────────────────────┤&#xa;│ 0x0034 │ [padding]            │ pad   │ 204B │ 0x00...  │&#xa;│ ...    │ (bincode pads to 256 bytes total)            │&#xa;│ 0x00FF │                                                │&#xa;└──────────────────────────────────────────────────────────┘&#xa;&#xa;Bincode Serialization Format:&#xa;- Uses variable-length integer encoding for some fields&#xa;- Fixed 256-byte output via padding&#xa;- Deserialization: bincode::deserialize(&amp;bytes[0..256])&#xa;&#xa;Hex Dump Example (first 64 bytes):&#xa;00000000: 4D 4F 4E 47 4F 4C 54 45  02 00 00 00 00 10 00 00  MONGOLTE........&#xa;00000010: 03 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  ................&#xa;00000020: 00 00 00 00 00 00 00 00  B0 A4 01 00 00 00 00 00  ................&#xa;00000030: 98 3A 00 00 00 00 00 00  00 00 00 00 00 00 00 00  .:..............&#xa;&#xa;Cache Line Alignment (x86_64):&#xa;- Header fits in 4 cache lines (64 bytes each)&#xa;- First cache line: magic + version + page_size + collection_count&#xa;- Optimized for fast header reads&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=9;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="90" y="210" width="880" height="690" as="geometry" />
        </mxCell>

        <!-- CollectionMeta Memory Layout -->
        <mxCell id="collmeta_memory" value="CollectionMeta Struct - Memory Layout" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="1020" y="185" width="1100" height="730" as="geometry" />
        </mxCell>

        <mxCell id="collmeta_memory_detail" value="&#xa;Rust Definition:&#xa;#[derive(Serialize, Deserialize, Clone, Debug)]&#xa;pub struct CollectionMeta {&#xa;    pub name: String,                                    // Heap-allocated&#xa;    pub document_count: u64,                             // Stack: 8 bytes&#xa;    pub data_offset: u64,                                // Stack: 8 bytes&#xa;    pub index_offset: u64,                               // Stack: 8 bytes&#xa;    pub last_id: i64,                                    // Stack: 8 bytes&#xa;    #[serde(with = &quot;crate::storage::catalog_serde&quot;)]&#xa;    pub document_catalog: HashMap&lt;DocumentId, u64&gt;,      // Heap-allocated&#xa;}&#xa;&#xa;Memory Layout (x86_64):&#xa;┌──────────────────────────────────────────────────────────────────┐&#xa;│ Field              │ Offset │ Size    │ Location │ Notes         │&#xa;├──────────────────────────────────────────────────────────────────┤&#xa;│ name               │   0x00 │ 24 bytes│ Stack    │ String = {    │&#xa;│   - ptr: *const u8 │   0x00 │  8 bytes│          │   ptr,        │&#xa;│   - len: usize     │   0x08 │  8 bytes│          │   len,        │&#xa;│   - cap: usize     │   0x10 │  8 bytes│          │   capacity }  │&#xa;│   Actual data      │   Heap │ Variable│ Heap     │ &quot;users&quot;\0    │&#xa;├──────────────────────────────────────────────────────────────────┤&#xa;│ document_count     │   0x18 │  8 bytes│ Stack    │ u64           │&#xa;├──────────────────────────────────────────────────────────────────┤&#xa;│ data_offset        │   0x20 │  8 bytes│ Stack    │ u64 = 256     │&#xa;├──────────────────────────────────────────────────────────────────┤&#xa;│ index_offset       │   0x28 │  8 bytes│ Stack    │ u64 = 256     │&#xa;├──────────────────────────────────────────────────────────────────┤&#xa;│ last_id            │   0x30 │  8 bytes│ Stack    │ i64           │&#xa;├──────────────────────────────────────────────────────────────────┤&#xa;│ document_catalog   │   0x38 │ 48 bytes│ Stack    │ HashMap = {   │&#xa;│   HashMap internals│        │         │          │   buckets,    │&#xa;│   - buckets: *mut  │   0x38 │  8 bytes│          │   len,        │&#xa;│   - len: usize     │   0x40 │  8 bytes│          │   capacity }  │&#xa;│   - cap: usize     │   0x48 │  8 bytes│          │               │&#xa;│   - [metadata]     │   0x50 │ 24 bytes│          │   hash state  │&#xa;│   Actual entries   │   Heap │ Variable│ Heap     │ Key-value     │&#xa;└──────────────────────────────────────────────────────────────────┘&#xa;&#xa;Total Stack Size: ~104 bytes (stack frame)&#xa;Heap Allocation:&#xa;  - name: 5-20 bytes typically (&quot;users&quot;, &quot;products&quot;, etc.)&#xa;  - document_catalog: 24 bytes/entry * N entries&#xa;    • Each entry: (DocumentId=16B, u64=8B) + hash overhead&#xa;    • For 1000 docs: ~24 KB heap&#xa;    • For 100K docs: ~2.4 MB heap&#xa;    • For 650K docs: ~15 MB heap&#xa;&#xa;JSON Serialization Format (for metadata persistence):&#xa;{&#xa;  &quot;name&quot;: &quot;users&quot;,&#xa;  &quot;document_count&quot;: 1000,&#xa;  &quot;data_offset&quot;: 256,&#xa;  &quot;index_offset&quot;: 256,&#xa;  &quot;last_id&quot;: 1000,&#xa;  &quot;document_catalog&quot;: [&#xa;    [0, 1, 256],      // [type_tag, id_value, offset]&#xa;    [0, 2, 512],&#xa;    [1, &quot;abc&quot;, 768],  // String ID&#xa;    [2, &quot;507f1f77bcf86cd799439011&quot;, 1024],  // ObjectId hex&#xa;    ...&#xa;  ]&#xa;}&#xa;&#xa;Serialized Size Estimation:&#xa;  - Base: ~100 bytes (name + counts)&#xa;  - Per document in catalog: ~25 bytes JSON&#xa;  - 1000 docs: ~25 KB&#xa;  - 100K docs: ~2.5 MB&#xa;  - 650K docs: ~16 MB&#xa;&#xa;HashMap Performance:&#xa;  - Load factor: 0.75 (default Rust HashMap)&#xa;  - Lookup: O(1) average, O(n) worst case&#xa;  - Insert: O(1) amortized&#xa;  - Resize triggers at 75% capacity&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=9;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="1030" y="210" width="1080" height="690" as="geometry" />
        </mxCell>

        <!-- DocumentId Memory Layout -->
        <mxCell id="docid_memory" value="DocumentId Enum - Memory Layout (Discriminated Union)" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="2160" y="185" width="1100" height="730" as="geometry" />
        </mxCell>

        <mxCell id="docid_memory_detail" value="&#xa;Rust Definition:&#xa;#[derive(Debug, Clone, PartialEq, Eq, Hash)]&#xa;#[derive(Serialize, Deserialize)]&#xa;#[serde(untagged)]  // No type tag in JSON&#xa;pub enum DocumentId {&#xa;    Int(i64),           // Tag 0, Data: 8 bytes&#xa;    String(String),     // Tag 1, Data: 24 bytes (ptr+len+cap)&#xa;    ObjectId([u8; 12]), // Tag 2, Data: 12 bytes&#xa;}&#xa;&#xa;Memory Layout (x86_64, Rust enum optimization):&#xa;┌───────────────────────────────────────────────────────────────┐&#xa;│ Discriminant (Tag) │ Offset │ Size │ Value                   │&#xa;├───────────────────────────────────────────────────────────────┤&#xa;│ Tag byte           │   0x00 │  1B  │ 0=Int, 1=String, 2=Oid │&#xa;│ [padding]          │   0x01 │  7B  │ 0x00 (alignment)       │&#xa;├───────────────────────────────────────────────────────────────┤&#xa;│ Data (union)       │   0x08 │ 24B  │ Largest variant        │&#xa;│   Int variant:     │        │      │                        │&#xa;│     value: i64     │   0x08 │  8B  │ Example: 42            │&#xa;│     [unused]       │   0x10 │ 16B  │ 0x00                   │&#xa;│   String variant:  │        │      │                        │&#xa;│     ptr: *const u8 │   0x08 │  8B  │ Heap pointer           │&#xa;│     len: usize     │   0x10 │  8B  │ String length          │&#xa;│     cap: usize     │   0x18 │  8B  │ Capacity               │&#xa;│   ObjectId variant:│        │      │                        │&#xa;│     bytes: [u8;12] │   0x08 │ 12B  │ 12-byte binary         │&#xa;│     [unused]       │   0x14 │ 12B  │ 0x00                   │&#xa;└───────────────────────────────────────────────────────────────┘&#xa;&#xa;Total Size: 32 bytes (1 tag + 7 pad + 24 data)&#xa;Alignment: 8 bytes (largest field alignment)&#xa;&#xa;Variant Examples:&#xa;&#xa;1. DocumentId::Int(42):&#xa;   Hex: 00 00 00 00 00 00 00 00  2A 00 00 00 00 00 00 00&#xa;        └─tag=0──┘ └──padding──┘  └────value=42────┘&#xa;        00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00&#xa;        └──────unused (16B)──────────────────────────┘&#xa;&#xa;2. DocumentId::String(&quot;abc&quot;):&#xa;   Stack:&#xa;   Hex: 01 00 00 00 00 00 00 00  A0 B1 C2 D3 E4 F5 06 07&#xa;        └─tag=1──┘ └──padding──┘  └──ptr to heap──────┘&#xa;        03 00 00 00 00 00 00 00  03 00 00 00 00 00 00 00&#xa;        └──len=3─────────────┘  └──cap=3─────────────┘&#xa;   Heap (at 0x07060F5E4D3C2B1A0):&#xa;   Hex: 61 62 63  (&quot;abc&quot;)&#xa;&#xa;3. DocumentId::ObjectId([...]):&#xa;   Hex: 02 00 00 00 00 00 00 00  50 7F 1F 77 BC F8 6C D7&#xa;        └─tag=2──┘ └──padding──┘  └──ObjectId bytes──┘&#xa;        99 43 90 11 00 00 00 00  00 00 00 00 00 00 00 00&#xa;        └──(continued)───────┘  └────unused (12B)────┘&#xa;&#xa;JSON Serialization (untagged):&#xa;  Int(42)           → 42&#xa;  String(&quot;abc&quot;)     → &quot;abc&quot;&#xa;  ObjectId([...])   → {&quot;$oid&quot;: &quot;507f1f77bcf86cd799439011&quot;}&#xa;&#xa;Custom catalog_serde Serialization (tagged):&#xa;  Int(42)           → [0, 42, offset]&#xa;  String(&quot;abc&quot;)     → [1, &quot;abc&quot;, offset]&#xa;  ObjectId([...])   → [2, &quot;507f1f77...&quot;, offset]&#xa;&#xa;Hash Implementation:&#xa;  - Uses DefaultHasher (SipHash-1-3)&#xa;  - Int: hash(tag=0, value)&#xa;  - String: hash(tag=1, bytes)&#xa;  - ObjectId: hash(tag=2, bytes)&#xa;  - Collision rate: ~1 / 2^64&#xa;&#xa;Performance:&#xa;  - Clone: O(1) for Int/ObjectId, O(n) for String&#xa;  - Hash: O(1) for Int/ObjectId, O(n) for String&#xa;  - Eq: O(1) for Int/ObjectId, O(n) for String&#xa;  - Size impact in HashMap: 32 bytes/entry + 8 bytes offset = 40 bytes&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=9;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="2170" y="210" width="1080" height="690" as="geometry" />
        </mxCell>

        <!-- StorageEngine Memory Layout -->
        <mxCell id="storage_memory" value="StorageEngine Struct - Complete Memory Layout" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="3300" y="185" width="1420" height="730" as="geometry" />
        </mxCell>

        <mxCell id="storage_memory_detail" value="&#xa;Rust Definition:&#xa;pub struct StorageEngine {&#xa;    pub(crate) file: File,                                    // Stack: 8 bytes (file descriptor)&#xa;    pub(crate) file_path: String,                             // Stack: 24 bytes (ptr+len+cap)&#xa;    pub(crate) header: Header,                                // Stack: 256 bytes (inline)&#xa;    pub(crate) collections: HashMap&lt;String, CollectionMeta&gt;,  // Stack: 48 bytes (HashMap struct)&#xa;    pub(crate) mmap: Option&lt;MmapMut&gt;,                         // Stack: 16 bytes (Option&lt;ptr&gt;)&#xa;}&#xa;&#xa;Memory Layout:&#xa;┌────────────────────────────────────────────────────────────────────────┐&#xa;│ Field          │ Offset │ Stack Size │ Heap Size    │ Total          │&#xa;├────────────────────────────────────────────────────────────────────────┤&#xa;│ file           │   0x00 │      8B    │      0       │     8B         │&#xa;│   - fd: RawFd  │   0x00 │      4B    │              │ (file descr.)  │&#xa;│   - [internal] │   0x04 │      4B    │              │                │&#xa;├────────────────────────────────────────────────────────────────────────┤&#xa;│ file_path      │   0x08 │     24B    │ ~20-50B      │    ~44B        │&#xa;│   - ptr        │   0x08 │      8B    │              │                │&#xa;│   - len        │   0x10 │      8B    │              │                │&#xa;│   - cap        │   0x18 │      8B    │              │                │&#xa;│   [heap data]  │   Heap │            │ &quot;db.mlite&quot;  │                │&#xa;├────────────────────────────────────────────────────────────────────────┤&#xa;│ header         │   0x20 │    256B    │      0       │   256B         │&#xa;│   (inline)     │        │            │              │ (no heap)      │&#xa;├────────────────────────────────────────────────────────────────────────┤&#xa;│ collections    │  0x120 │     48B    │ N * ~2.5MB   │  Variable      │&#xa;│   HashMap      │        │            │              │                │&#xa;│   For 3 colls: │        │            │              │                │&#xa;│     - users    │   Heap │            │ ~15 MB       │ (650K docs)    │&#xa;│     - products │   Heap │            │ ~100 KB      │ (5K docs)      │&#xa;│     - orders   │   Heap │            │ ~50 KB       │ (2K docs)      │&#xa;│   Total heap:  │        │            │ ~15.15 MB    │                │&#xa;├────────────────────────────────────────────────────────────────────────┤&#xa;│ mmap           │  0x150 │     16B    │ File size    │  Variable      │&#xa;│   Option&lt;Ptr&gt;  │        │            │              │                │&#xa;│   - Some(ptr)  │  0x150 │      8B    │              │ (if &lt;1GB)      │&#xa;│   - discrimin. │  0x158 │      8B    │              │                │&#xa;│   [mmap region]│   OS   │            │ ~564 MB      │ (650K docs)    │&#xa;└────────────────────────────────────────────────────────────────────────┘&#xa;&#xa;Total Stack Size: ~360 bytes&#xa;Total Heap Size: ~15-20 MB (for 650K docs across 3 collections)&#xa;Total Virtual Memory (mmap): ~564 MB (file size, not resident)&#xa;&#xa;Actual Memory Usage Example (650K docs):&#xa;  Stack:    360 bytes&#xa;  Heap:     15.15 MB (catalogs)&#xa;  Resident: ~50-100 MB (OS pages mmap actually in RAM)&#xa;  Virtual:  564 MB (file mmap'd, but lazy-loaded)&#xa;  Total:    ~65-115 MB resident RAM&#xa;&#xa;Thread Safety Wrapper:&#xa;Arc&lt;RwLock&lt;StorageEngine&gt;&gt; adds:&#xa;  - Arc: 16 bytes (2 * AtomicUsize: ref_count + weak_count)&#xa;  - RwLock: 8 bytes (lock state)&#xa;  Total wrapper overhead: 24 bytes&#xa;&#xa;Memory Map (mmap) Details:&#xa;  When file &lt; 1GB:&#xa;    mmap = Some(MmapMut::map_mut(&amp;file))&#xa;    - Flags: MAP_SHARED | MAP_POPULATE&#xa;    - Protection: PROT_READ | PROT_WRITE&#xa;    - Lazy loading: Pages loaded on first access&#xa;    - Benefits:&#xa;      • Zero-copy reads&#xa;      • OS-managed caching&#xa;      • Fast random access&#xa;    - Cost:&#xa;      • Virtual address space = file size&#xa;      • Resident RAM = accessed pages only&#xa;&#xa;  When file &gt;= 1GB:&#xa;    mmap = None&#xa;    Falls back to regular file I/O:&#xa;      file.seek(SeekFrom::Start(offset))?;&#xa;      file.read_exact(&amp;mut buffer)?;&#xa;    - Slower (syscall per read)&#xa;    - No virtual address space consumption&#xa;    - Manual buffering needed&#xa;&#xa;Performance Impact:&#xa;  Small DB (100 docs, 10 KB):&#xa;    - Stack: 360B&#xa;    - Heap: ~10 KB&#xa;    - Mmap: 10 KB&#xa;    - Total: ~11 KB resident&#xa;&#xa;  Medium DB (10K docs, 1 MB):&#xa;    - Stack: 360B&#xa;    - Heap: ~300 KB&#xa;    - Mmap: 1 MB&#xa;    - Total: ~1.3 MB resident&#xa;&#xa;  Large DB (650K docs, 564 MB):&#xa;    - Stack: 360B&#xa;    - Heap: ~15 MB&#xa;    - Mmap: 564 MB virtual, ~50-100 MB resident&#xa;    - Total: ~65-115 MB resident&#xa;&#xa;Cache Line Analysis (x86_64, 64-byte cache lines):&#xa;  - Header (256B): 4 cache lines&#xa;  - Collections HashMap: Scattered across heap&#xa;  - Hot path fields (file, header): First 2 cache lines&#xa;  - Cold path (mmap): Last cache line&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=9;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="3310" y="210" width="1400" height="690" as="geometry" />
        </mxCell>

        <!-- ==================== SECTION 2: FILE FORMAT BYTE-LEVEL ==================== -->
        <mxCell id="fileformat_section" value="SECTION 2: FILE FORMAT - BYTE-LEVEL SPECIFICATION" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#000000;strokeColor=#000000;fontColor=#FFFFFF;verticalAlign=top;fontSize=20;fontStyle=1;align=center;" vertex="1" parent="1">
          <mxGeometry x="40" y="980" width="4720" height="900" as="geometry" />
        </mxCell>

        <!-- Complete File Example -->
        <mxCell id="file_example" value="Complete .mlite File Example - Byte-by-Byte Breakdown" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="80" y="1025" width="2300" height="830" as="geometry" />
        </mxCell>

        <mxCell id="file_example_detail" value="&#xa;Example Database: 3 documents in &quot;users&quot; collection&#xa;  Doc 1: {&quot;_id&quot;: 1, &quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 30}&#xa;  Doc 2: {&quot;_id&quot;: 2, &quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 25}&#xa;  Doc 3: {&quot;_id&quot;: 3, &quot;name&quot;: &quot;Charlie&quot;, &quot;age&quot;: 35}&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;OFFSET 0x0000: HEADER (256 bytes)&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;Hex Dump (first 128 bytes):&#xa;00000000: 4D 4F 4E 47 4F 4C 54 45  02 00 00 00 00 10 00 00  │MONGOLTE│........│&#xa;          └─────magic─────────┘  └─ver=2─┘ └page=4096┘&#xa;00000010: 01 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │........│........│&#xa;          └─coll_ct=1┘ └free_list─────┘&#xa;00000020: 00 00 00 00 00 00 00 00  C8 01 00 00 00 00 00 00  │........│........│&#xa;          └idx_offset────────┘  └meta_offset=0x1C8┘&#xa;00000030: A5 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │........│........│&#xa;          └meta_size=0xA5───┘  └────padding───────┘&#xa;00000040: 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │........│........│&#xa;00000050: 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │........│........│&#xa;... (padding continues to byte 255)&#xa;000000F0: 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  │........│........│&#xa;&#xa;Parsed Header:&#xa;  magic            = &quot;MONGOLTE&quot; (0x4D4F4E474F4C5445)&#xa;  version          = 2 (dynamic metadata)&#xa;  page_size        = 4096 (0x1000)&#xa;  collection_count = 1&#xa;  free_list_head   = 0&#xa;  index_section_offset = 0&#xa;  metadata_offset  = 456 (0x1C8) ← Metadata starts here&#xa;  metadata_size    = 165 bytes (0xA5)&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;OFFSET 0x0100: DOCUMENT DATA (starting at 256 = HEADER_SIZE)&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;Document 1: {&quot;_id&quot;: 1, &quot;_collection&quot;: &quot;users&quot;, &quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 30}&#xa;JSON: {&quot;_id&quot;:1,&quot;_collection&quot;:&quot;users&quot;,&quot;name&quot;:&quot;Alice&quot;,&quot;age&quot;:30}&#xa;JSON size: 56 bytes&#xa;&#xa;00000100: 38 00 00 00 7B 22 5F 69  64 22 3A 31 2C 22 5F 63  │8...{&quot;_i│d&quot;:1,&quot;_c│&#xa;          └len=56──┘ └──JSON starts──────────────────┘&#xa;00000110: 6F 6C 6C 65 63 74 69 6F  6E 22 3A 22 75 73 65 72  │ollectio│n&quot;:&quot;user│&#xa;00000120: 73 22 2C 22 6E 61 6D 65  22 3A 22 41 6C 69 63 65  │s&quot;,&quot;name│&quot;:&quot;Alice│&#xa;00000130: 22 2C 22 61 67 65 22 3A  33 30 7D                 │&quot;,&quot;age&quot;:│30}     │&#xa;                                                   └─JSON ends─┘&#xa;&#xa;Catalog entry: DocumentId::Int(1) → offset 0x100 (256)&#xa;&#xa;Document 2: {&quot;_id&quot;: 2, &quot;_collection&quot;: &quot;users&quot;, &quot;name&quot;: &quot;Bob&quot;, &quot;age&quot;: 25}&#xa;JSON: {&quot;_id&quot;:2,&quot;_collection&quot;:&quot;users&quot;,&quot;name&quot;:&quot;Bob&quot;,&quot;age&quot;:25}&#xa;JSON size: 53 bytes&#xa;&#xa;0000013C: 35 00 00 00 7B 22 5F 69  64 22 3A 32 2C 22 5F 63  │5...{&quot;_i│d&quot;:2,&quot;_c│&#xa;          └len=53──┘&#xa;0000014C: 6F 6C 6C 65 63 74 69 6F  6E 22 3A 22 75 73 65 72  │ollectio│n&quot;:&quot;user│&#xa;0000015C: 73 22 2C 22 6E 61 6D 65  22 3A 22 42 6F 62 22 2C  │s&quot;,&quot;name│&quot;:&quot;Bob&quot;,│&#xa;0000016C: 22 61 67 65 22 3A 32 35  7D                       │&quot;age&quot;:25│}       │&#xa;&#xa;Catalog entry: DocumentId::Int(2) → offset 0x13C (316)&#xa;&#xa;Document 3: {&quot;_id&quot;: 3, &quot;_collection&quot;: &quot;users&quot;, &quot;name&quot;: &quot;Charlie&quot;, &quot;age&quot;: 35}&#xa;JSON: {&quot;_id&quot;:3,&quot;_collection&quot;:&quot;users&quot;,&quot;name&quot;:&quot;Charlie&quot;,&quot;age&quot;:35}&#xa;JSON size: 60 bytes&#xa;&#xa;00000171: 3C 00 00 00 7B 22 5F 69  64 22 3A 33 2C 22 5F 63  │&lt;...{&quot;_i│d&quot;:3,&quot;_c│&#xa;          └len=60──┘&#xa;00000181: 6F 6C 6C 65 63 74 69 6F  6E 22 3A 22 75 73 65 72  │ollectio│n&quot;:&quot;user│&#xa;00000191: 73 22 2C 22 6E 61 6D 65  22 3A 22 43 68 61 72 6C  │s&quot;,&quot;name│&quot;:&quot;Charl│&#xa;000001A1: 69 65 22 2C 22 61 67 65  22 3A 33 35 7D           │ie&quot;,&quot;age│&quot;:35}    │&#xa;&#xa;Catalog entry: DocumentId::Int(3) → offset 0x171 (369)&#xa;&#xa;Document data ends at: 0x171 + 4 + 60 = 0x1AD (429)&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;OFFSET 0x01C8 (456): METADATA (at file end, dynamic position)&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;Metadata Format:&#xa;  [u32: collection_count]&#xa;  For each collection:&#xa;    [u32: json_length]&#xa;    [JSON bytes: CollectionMeta]&#xa;&#xa;000001C8: 01 00 00 00 A1 00 00 00  7B 22 6E 61 6D 65 22 3A  │....¡...│{&quot;name&quot;:│&#xa;          └coll_ct=1┘ └json_len=161┘ └─JSON starts──────┘&#xa;000001D8: 22 75 73 65 72 73 22 2C  22 64 6F 63 75 6D 65 6E  │&quot;users&quot;,│&quot;documen│&#xa;000001E8: 74 5F 63 6F 75 6E 74 22  3A 33 2C 22 64 61 74 61  │t_count&quot;│:3,&quot;data│&#xa;000001F8: 5F 6F 66 66 73 65 74 22  3A 32 35 36 2C 22 69 6E  │_offset&quot;│:256,&quot;in│&#xa;00000208: 64 65 78 5F 6F 66 66 73  65 74 22 3A 32 35 36 2C  │dex_offs│et&quot;:256,│&#xa;00000218: 22 6C 61 73 74 5F 69 64  22 3A 33 2C 22 64 6F 63  │&quot;last_id│&quot;:3,&quot;doc│&#xa;00000228: 75 6D 65 6E 74 5F 63 61  74 61 6C 6F 67 22 3A 5B  │ument_ca│talog&quot;:[│&#xa;00000238: 5B 30 2C 31 2C 32 35 36  5D 2C 5B 30 2C 32 2C 33  │[0,1,256│],[0,2,3│&#xa;          └[type=0,id=1,offset=256]─┘ └[type=0,id=2,offset=316]─┘&#xa;00000248: 31 36 5D 2C 5B 30 2C 33  2C 33 36 39 5D 5D 7D     │16],[0,3│,369]]}  │&#xa;          └─────────────────┘ └[type=0,id=3,offset=369]─┘ └─JSON ends─┘&#xa;&#xa;Parsed Metadata:&#xa;{&#xa;  &quot;name&quot;: &quot;users&quot;,&#xa;  &quot;document_count&quot;: 3,&#xa;  &quot;data_offset&quot;: 256,&#xa;  &quot;index_offset&quot;: 256,&#xa;  &quot;last_id&quot;: 3,&#xa;  &quot;document_catalog&quot;: [&#xa;    [0, 1, 256],    // DocumentId::Int(1) at offset 256&#xa;    [0, 2, 316],    // DocumentId::Int(2) at offset 316&#xa;    [0, 3, 369]     // DocumentId::Int(3) at offset 369&#xa;  ]&#xa;}&#xa;&#xa;Metadata size: 4 (count) + 4 (len) + 161 (JSON) = 169 bytes&#xa;File size: 456 (metadata_offset) + 169 = 625 bytes total&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;FILE SUMMARY&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;Total file size: 625 bytes&#xa;Breakdown:&#xa;  - Header:      256 bytes (41.0%)&#xa;  - Documents:   173 bytes (27.7%) [3 docs * ~57 bytes avg]&#xa;  - Gap:          27 bytes (4.3%)  [unused space between last doc and metadata]&#xa;  - Metadata:    169 bytes (27.0%)&#xa;&#xa;Efficiency: 27.7% actual data, rest is overhead&#xa;(Overhead decreases as document count increases)&#xa;&#xa;For comparison, Version 1 would have been:&#xa;  - Header:      256 bytes&#xa;  - Metadata:    10 MB (fixed)&#xa;  - Documents:   173 bytes&#xa;  Total: ~10 MB (99.998% waste!)&#xa;&#xa;Version 2 saves: 10 MB - 625 bytes = 9,999,375 bytes (99.99% reduction!)&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=8;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="90" y="1050" width="2280" height="790" as="geometry" />
        </mxCell>

        <!-- Document Format Details -->
        <mxCell id="doc_format" value="Document Storage Format - Detailed Specification" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="2420" y="1025" width="2300" height="830" as="geometry" />
        </mxCell>

        <mxCell id="doc_format_detail" value="&#xa;Document Serialization Pipeline:&#xa;&#xa;1. User Input (Python):&#xa;   collection.insert_one({&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 30})&#xa;&#xa;2. PyO3 Conversion (python_to_json):&#xa;   PyDict → serde_json::Value&#xa;   Value::Object({&#xa;     &quot;name&quot;: Value::String(&quot;Alice&quot;),&#xa;     &quot;age&quot;: Value::Number(30)&#xa;   })&#xa;&#xa;3. Field Injection (collection.rs):&#xa;   doc[&quot;_id&quot;] = generate_id() // DocumentId::Int(1)&#xa;   doc[&quot;_collection&quot;] = &quot;users&quot;&#xa;   Result:&#xa;   {&#xa;     &quot;_id&quot;: 1,&#xa;     &quot;_collection&quot;: &quot;users&quot;,&#xa;     &quot;name&quot;: &quot;Alice&quot;,&#xa;     &quot;age&quot;: 30&#xa;   }&#xa;&#xa;4. JSON Serialization (serde_json):&#xa;   let doc_bytes = serde_json::to_vec(&amp;doc)?;&#xa;   Result (compact, no whitespace):&#xa;   {&quot;_id&quot;:1,&quot;_collection&quot;:&quot;users&quot;,&quot;name&quot;:&quot;Alice&quot;,&quot;age&quot;:30}&#xa;   Size: 56 bytes&#xa;&#xa;5. Length Prefix Encoding:&#xa;   let len = doc_bytes.len() as u32;  // 56&#xa;   let len_bytes = len.to_le_bytes(); // [0x38, 0x00, 0x00, 0x00]&#xa;&#xa;6. Write to File:&#xa;   file.write_all(&amp;len_bytes)?;  // 4 bytes&#xa;   file.write_all(&amp;doc_bytes)?;  // 56 bytes&#xa;   Total written: 60 bytes&#xa;&#xa;7. Catalog Update (in-memory):&#xa;   catalog.insert(DocumentId::Int(1), offset);&#xa;   // offset = file position before write = 256&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;DOCUMENT READ PIPELINE (find operation)&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;1. Lookup in Catalog:&#xa;   let offset = catalog.get(&amp;DocumentId::Int(1))?; // 256&#xa;&#xa;2. Read from File (mmap path):&#xa;   if let Some(ref mmap) = self.mmap {&#xa;       let offset_usize = offset as usize; // 256&#xa;       &#xa;       // Read length prefix (4 bytes)&#xa;       let len_bytes = &amp;mmap[256..260];&#xa;       // [0x38, 0x00, 0x00, 0x00]&#xa;       let len = u32::from_le_bytes([&#xa;           len_bytes[0], len_bytes[1], len_bytes[2], len_bytes[3]&#xa;       ]) as usize; // 56&#xa;       &#xa;       // Read document data (56 bytes)&#xa;       let data = &amp;mmap[260..316]; // mmap[260..260+56]&#xa;       // {&quot;_id&quot;:1,&quot;_collection&quot;:&quot;users&quot;,&quot;name&quot;:&quot;Alice&quot;,&quot;age&quot;:30}&#xa;       &#xa;       data.to_vec()&#xa;   }&#xa;&#xa;3. JSON Deserialization:&#xa;   let doc: Value = serde_json::from_slice(&amp;data)?;&#xa;   // Value::Object(...)&#xa;&#xa;4. Filter Check (query.rs):&#xa;   if matches_filter(&amp;doc, &amp;filter)? {&#xa;       // Example filter: {&quot;age&quot;: {&quot;$gt&quot;: 25}}&#xa;       let doc_age = doc.get(&quot;age&quot;).and_then(|v| v.as_i64()); // Some(30)&#xa;       let filter_age = filter[&quot;age&quot;][&quot;$gt&quot;].as_i64(); // Some(25)&#xa;       if doc_age &gt; filter_age { // 30 &gt; 25 = true&#xa;           results.push(doc);&#xa;       }&#xa;   }&#xa;&#xa;5. Return to Python (json_to_python):&#xa;   Value::Object → PyDict&#xa;   Python receives: {&quot;_id&quot;: 1, &quot;_collection&quot;: &quot;users&quot;, &quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 30}&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;PERFORMANCE ANALYSIS&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;Insert Performance (per document):&#xa;  1. Catalog lookup (HashMap):         O(1) ~10-50 ns&#xa;  2. ID generation:                    O(1) ~5-20 ns&#xa;  3. JSON serialization:               O(n) ~500 ns - 5 μs (n = doc size)&#xa;  4. File write (append):              O(1) ~1-10 μs (buffered)&#xa;  5. Catalog insert:                   O(1) ~10-50 ns&#xa;  Total: ~1.5-15 μs per document (average ~5 μs)&#xa;  &#xa;  Observed throughput: 11,505 docs/sec = 86 μs/doc&#xa;  (Includes Python overhead, PyO3 conversion, WAL writes)&#xa;&#xa;Find Performance (per document):&#xa;  1. Catalog iteration:                O(n) n = catalog size&#xa;  2. Offset lookup:                    O(1) ~10 ns&#xa;  3. mmap read:                        O(1) ~50-200 ns (cache hit)&#xa;                                           ~1-10 μs (cache miss, page fault)&#xa;  4. JSON deserialization:             O(n) ~500 ns - 5 μs (n = doc size)&#xa;  5. Filter matching:                  O(m) ~50-500 ns (m = filter complexity)&#xa;  Total per doc: ~1-15 μs&#xa;  &#xa;  Full scan (50K docs): ~50-750 ms&#xa;  With index: ~1-100 μs (future optimization)&#xa;&#xa;Storage Overhead:&#xa;  Per document:&#xa;    - Length prefix: 4 bytes&#xa;    - JSON overhead: ~10-30% (field names, syntax)&#xa;    - Catalog entry: 40 bytes (DocumentId + offset + HashMap overhead)&#xa;  &#xa;  Example:&#xa;    Raw data: {&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 30} = ~14 bytes (compact binary)&#xa;    JSON: {&quot;name&quot;:&quot;Alice&quot;,&quot;age&quot;:30} = 26 bytes&#xa;    Stored: 4 (len) + 26 (JSON) + 40 (catalog) = 70 bytes&#xa;    Overhead: 70 / 14 = 5x (JSON + metadata overhead)&#xa;&#xa;Compression Potential (future):&#xa;  - zstd compression: ~50-70% size reduction on JSON&#xa;  - Dictionary compression: ~80% for repeated field names&#xa;  - BSON binary: ~20-40% smaller than JSON&#xa;&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;EDGE CASES &amp; ERROR HANDLING&#xa;═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════&#xa;&#xa;1. Document Size Limits:&#xa;   - Length prefix: u32 = 4 GB max per document&#xa;   - Practical limit: ~100 MB (JSON parsing memory)&#xa;   - Recommendation: Split large documents into chunks&#xa;&#xa;2. Offset Overflow:&#xa;   - Offsets: u64 = 16 EB (exabytes) max file size&#xa;   - Filesystem limits:&#xa;     • ext4: 16 TB max file&#xa;     • NTFS: 256 TB max file&#xa;     • XFS: 8 EB max file&#xa;   - No overflow risk in practice&#xa;&#xa;3. Corrupted Length Prefix:&#xa;   - Detection: len &gt; remaining file size&#xa;   - Error: MongoLiteError::Corruption(&quot;Invalid document length&quot;)&#xa;   - Recovery: Skip to next known offset from catalog&#xa;&#xa;4. Invalid UTF-8 in JSON:&#xa;   - serde_json rejects invalid UTF-8&#xa;   - Error: MongoLiteError::Deserialization(&quot;Invalid UTF-8&quot;)&#xa;   - Prevention: Validate at insert time&#xa;&#xa;5. Concurrent Writes (multi-threaded):&#xa;   - Protection: RwLock (write lock = exclusive)&#xa;   - File append is NOT atomic across writes&#xa;   - Solution: Single writer lock ensures serialization&#xa;&#xa;6. Partial Writes (crash during write):&#xa;   - WAL ensures durability:&#xa;     1. Write to WAL first (fsync)&#xa;     2. Write to data file&#xa;     3. If crash: replay WAL on recovery&#xa;   - Incomplete document: Ignored if not in catalog&#xa;   - Catalog: Rebuilt from WAL on recovery&#xa;&#xa;7. mmap Failures (&gt;1GB files):&#xa;   - Fallback to regular file I/O&#xa;   - Performance impact: ~2-10x slower reads&#xa;   - No correctness impact&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=8;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="2430" y="1050" width="2280" height="790" as="geometry" />
        </mxCell>

        <!-- ==================== SECTION 3: ALGORITHMS WITH PSEUDOCODE ==================== -->
        <mxCell id="algorithms_section" value="SECTION 3: CORE ALGORITHMS - COMPLETE PSEUDOCODE" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#000000;strokeColor=#000000;fontColor=#FFFFFF;verticalAlign=top;fontSize=20;fontStyle=1;align=center;" vertex="1" parent="1">
          <mxGeometry x="40" y="1920" width="4720" height="1100" as="geometry" />
        </mxCell>

        <!-- flush_metadata Algorithm -->
        <mxCell id="algo_flush_metadata" value="Algorithm: flush_metadata() - Dynamic Metadata Positioning" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="80" y="1965" width="1500" height="1030" as="geometry" />
        </mxCell>

        <mxCell id="algo_flush_metadata_code" value="&#xa;pub(crate) fn flush_metadata(&amp;mut self) -&gt; Result&lt;()&gt; {&#xa;    // ┌─────────────────────────────────────────────────────────────┐&#xa;    // │ ALGORITHM: Dynamic Metadata Flush (Version 2)              │&#xa;    // │                                                             │&#xa;    // │ Goal: Write metadata at END of file after all documents     │&#xa;    // │ Challenge: Find exact end of document data dynamically      │&#xa;    // │ Solution: Scan catalog for highest offset, read last doc   │&#xa;    // │                                                             │&#xa;    // │ Time Complexity: O(n) where n = total documents             │&#xa;    // │ Space Complexity: O(m) where m = metadata size              │&#xa;    // │ I/O Operations: 1 read (last doc), 2 writes (metadata+hdr)  │&#xa;    // └─────────────────────────────────────────────────────────────┘&#xa;&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 1: Update Collection Metadata (in-memory)&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    let data_offset = HEADER_SIZE; // 256&#xa;    &#xa;    for meta in self.collections.values_mut() {&#xa;        meta.data_offset = data_offset;   // Reset to HEADER_SIZE&#xa;        meta.index_offset = data_offset;  // (future use)&#xa;    }&#xa;    // Cost: O(c) where c = collection count (typically &lt;10)&#xa;    &#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 2: Serialize Metadata to Buffer&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    let mut metadata_buffer = Cursor::new(Vec::new());&#xa;    Self::write_metadata_body(&amp;mut metadata_buffer, &amp;self.collections)?;&#xa;    // write_metadata_body():&#xa;    //   1. Write collection_count (4 bytes)&#xa;    //   2. For each collection:&#xa;    //        a. Serialize to JSON: serde_json::to_vec(meta)&#xa;    //        b. Write length prefix (4 bytes)&#xa;    //        c. Write JSON bytes&#xa;    // Cost: O(m) where m = metadata size&#xa;    //       For 650K docs: ~15 MB metadata, ~50-100 ms&#xa;    &#xa;    let metadata_bytes = metadata_buffer.into_inner();&#xa;    let metadata_size = metadata_bytes.len() as u64;&#xa;    &#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 3: Find End of Document Data (Critical!)&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    // Step 3a: Scan all catalogs for maximum offset&#xa;    let mut max_doc_offset: u64 = HEADER_SIZE; // Start at 256&#xa;    &#xa;    for coll_meta in self.collections.values() {&#xa;        for &amp;doc_offset in coll_meta.document_catalog.values() {&#xa;            if doc_offset &gt; max_doc_offset {&#xa;                max_doc_offset = doc_offset;&#xa;                // Example: doc at offset 107,440&#xa;            }&#xa;        }&#xa;    }&#xa;    // Cost: O(n) where n = total documents across all collections&#xa;    //       For 650K docs: ~10-50 ms (in-memory HashMap iteration)&#xa;    &#xa;    // Step 3b: Read last document's size&#xa;    let metadata_offset = if max_doc_offset &gt; HEADER_SIZE {&#xa;        // We found at least one document&#xa;        &#xa;        // Seek to the last document's offset&#xa;        self.file.seek(SeekFrom::Start(max_doc_offset))?;&#xa;        // File position now at: 107,440&#xa;        &#xa;        // Read 4-byte length prefix&#xa;        let mut len_bytes = [0u8; 4];&#xa;        if self.file.read_exact(&amp;mut len_bytes).is_ok() {&#xa;            let doc_len = u32::from_le_bytes(len_bytes) as u64;&#xa;            // Example: doc_len = 256 bytes&#xa;            &#xa;            // Calculate metadata position:&#xa;            // offset + length_prefix + document_data&#xa;            max_doc_offset + 4 + doc_len&#xa;            // = 107,440 + 4 + 256 = 107,700&#xa;        } else {&#xa;            // Failed to read (corrupted file?)&#xa;            // Fallback: use current file size&#xa;            self.file.metadata()?.len()&#xa;        }&#xa;    } else {&#xa;        // No documents yet, metadata right after header&#xa;        HEADER_SIZE // 256&#xa;    };&#xa;    // Cost: 1 seek + 1 read (4 bytes) = ~1-10 μs&#xa;    &#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 4: Truncate File to Remove Old Metadata&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    // Truncate file at exact end of document data&#xa;    self.file.set_len(metadata_offset)?;&#xa;    // This removes any old metadata that was previously at the end&#xa;    // Example: If old metadata was at offset 107,000, and new is 107,700,&#xa;    //          this truncates to 107,700 (removes old + any junk after)&#xa;    // Cost: 1 syscall (ftruncate), ~10-100 μs&#xa;    &#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 5: Write Metadata at File End&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    // Seek to metadata write position&#xa;    self.file.seek(SeekFrom::Start(metadata_offset))?;&#xa;    // File position: 107,700&#xa;    &#xa;    // Write metadata bytes&#xa;    self.file.write_all(&amp;metadata_bytes)?;&#xa;    // Writes ~15 MB for 650K docs&#xa;    // Cost: ~50-200 ms (depends on disk speed)&#xa;    &#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 6: Update Header with Metadata Pointers&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    self.header.metadata_offset = metadata_offset; // 107,700&#xa;    self.header.metadata_size = metadata_size;     // 15,000,000&#xa;    &#xa;    // Seek back to beginning&#xa;    self.file.seek(SeekFrom::Start(0))?;&#xa;    &#xa;    // Serialize header&#xa;    let header_bytes = bincode::serialize(&amp;self.header)&#xa;        .map_err(|e| MongoLiteError::Serialization(e.to_string()))?;&#xa;    // bincode pads to 256 bytes&#xa;    &#xa;    // Write header (overwrites old header)&#xa;    self.file.write_all(&amp;header_bytes)?;&#xa;    // Cost: ~10-50 μs&#xa;    &#xa;    // ═════════════════════════════════════════════════════════════&#xa;    // PHASE 7: Force Sync to Disk&#xa;    // ═════════════════════════════════════════════════════════════&#xa;    &#xa;    self.file.sync_all()?;&#xa;    // fsync() - ensures data is physically on disk&#xa;    // Cost: ~1-100 ms (depends on disk, I/O scheduler, cache)&#xa;    &#xa;    Ok(())&#xa;}&#xa;&#xa;// ═══════════════════════════════════════════════════════════════════&#xa;// PERFORMANCE PROFILE (650K documents, 15 MB metadata)&#xa;// ═══════════════════════════════════════════════════════════════════&#xa;//&#xa;// Phase 1: Update metadata         ~1 μs&#xa;// Phase 2: Serialize metadata      ~50-100 ms&#xa;// Phase 3a: Scan catalogs           ~10-50 ms&#xa;// Phase 3b: Read last doc size      ~10 μs&#xa;// Phase 4: Truncate file            ~100 μs&#xa;// Phase 5: Write metadata           ~50-200 ms&#xa;// Phase 6: Update header            ~50 μs&#xa;// Phase 7: Sync to disk             ~1-100 ms&#xa;// ───────────────────────────────────────────&#xa;// TOTAL:                            ~100-450 ms&#xa;//&#xa;// Amortized Cost (if flushing every 1000 inserts):&#xa;//   ~100-450 ms / 1000 = ~0.1-0.45 ms per insert&#xa;//&#xa;// With lazy flush (flush only on close):&#xa;//   ~100-450 ms once per session (negligible amortized cost)&#xa;// ═══════════════════════════════════════════════════════════════════&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=8;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="90" y="1990" width="1480" height="990" as="geometry" />
        </mxCell>

        <!-- Compaction Algorithm -->
        <mxCell id="algo_compaction" value="Algorithm: compact_with_config() - Chunked Compaction" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="1620" y="1965" width="1550" height="1030" as="geometry" />
        </mxCell>

        <mxCell id="algo_compaction_code" value="&#xa;pub fn compact_with_config(&amp;mut self, config: &amp;CompactionConfig) -&gt; Result&lt;CompactionStats&gt; {&#xa;    // ┌─────────────────────────────────────────────────────────────────────────┐&#xa;    // │ ALGORITHM: Chunked Compaction (Memory-Efficient)                       │&#xa;    // │                                                                         │&#xa;    // │ Goal: Remove tombstones and old versions, minimize memory usage         │&#xa;    // │ Challenge: 650K docs * 100 bytes = 65 MB in memory (too much!)         │&#xa;    // │ Solution: Process in chunks of 1000 docs, flush, clear memory, repeat  │&#xa;    // │                                                                         │&#xa;    // │ Time Complexity: O(n log n) where n = document count                   │&#xa;    // │   - Read all docs: O(n)                                                 │&#xa;    // │   - HashMap insert/lookup per chunk: O(chunk_size)                     │&#xa;    // │   - Write compacted docs: O(n)                                          │&#xa;    // │ Space Complexity: O(chunk_size) = O(1000) = constant!                  │&#xa;    // │ I/O: 1 full scan read + 1 full write + metadata r/w                    │&#xa;    // └─────────────────────────────────────────────────────────────────────────┘&#xa;&#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    // PHASE 1: Pre-Compaction Setup&#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    &#xa;    // CRITICAL: Flush metadata first to ensure header.metadata_offset is set!&#xa;    self.flush_metadata()?;&#xa;    // Why? We need to know where document data ends, so we don't read metadata as docs&#xa;    &#xa;    let temp_path = format!(&quot;{}.compact&quot;, self.file_path);&#xa;    let mut stats = CompactionStats::default();&#xa;    stats.size_before = self.file.metadata()?.len();&#xa;    &#xa;    // Determine scan boundary (Version 2: stop at metadata_offset)&#xa;    let file_len = if self.header.version &gt;= 2 &amp;&amp; self.header.metadata_offset &gt; 0 {&#xa;        self.header.metadata_offset // Example: 107,700 (don't read metadata!)&#xa;    } else {&#xa;        self.file_len()? // Version 1 fallback&#xa;    };&#xa;    &#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    // PHASE 2: Create Temporary Compacted File&#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    &#xa;    let mut new_file = OpenOptions::new()&#xa;        .read(true).write(true).create(true).truncate(true)&#xa;        .open(&amp;temp_path)?;&#xa;    &#xa;    // Write header only (NO metadata yet - docs start at HEADER_SIZE)&#xa;    new_file.seek(SeekFrom::Start(0))?;&#xa;    let header_bytes = bincode::serialize(&amp;self.header)?;&#xa;    new_file.write_all(&amp;header_bytes)?;&#xa;    &#xa;    // Position for document writes&#xa;    new_file.seek(SeekFrom::Start(HEADER_SIZE))?; // 256&#xa;    let mut write_offset = HEADER_SIZE;&#xa;    &#xa;    // Initialize new collection metadata (empty catalogs)&#xa;    let mut new_collections = self.collections.clone();&#xa;    for coll_meta in new_collections.values_mut() {&#xa;        coll_meta.data_offset = HEADER_SIZE;&#xa;        coll_meta.document_catalog.clear(); // Will rebuild from scratch&#xa;        coll_meta.document_count = 0;&#xa;    }&#xa;    &#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    // PHASE 3: Chunked Document Processing (Memory-Efficient!)&#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    &#xa;    // Per-collection document buffers (one HashMap per collection)&#xa;    let mut collection_docs: HashMap&lt;String, HashMap&lt;DocumentId, Value&gt;&gt; = HashMap::new();&#xa;    for coll_name in self.collections.keys() {&#xa;        collection_docs.insert(coll_name.clone(), HashMap::new());&#xa;    }&#xa;    &#xa;    let mut current_offset = HEADER_SIZE; // Start scan at 256&#xa;    let mut chunk_count = 0;&#xa;    &#xa;    // Scan through entire document data region&#xa;    while current_offset &lt; file_len {&#xa;        // ─────────────────────────────────────────────────────────────────────&#xa;        // Read Document from Old File&#xa;        // ─────────────────────────────────────────────────────────────────────&#xa;        match self.read_data(current_offset) {&#xa;            Ok(doc_bytes) =&gt; {&#xa;                stats.documents_scanned += 1;&#xa;                &#xa;                // Deserialize JSON&#xa;                if let Ok(doc) = serde_json::from_slice::&lt;Value&gt;(&amp;doc_bytes) {&#xa;                    // Extract collection name&#xa;                    let coll = doc.get(&quot;_collection&quot;)&#xa;                        .and_then(|v| v.as_str())&#xa;                        .unwrap_or(&quot;&quot;);&#xa;                    &#xa;                    if let Some(docs_by_id) = collection_docs.get_mut(coll) {&#xa;                        // Extract document ID&#xa;                        if let Some(id_val) = doc.get(&quot;_id&quot;) {&#xa;                            if let Ok(doc_id) = serde_json::from_value::&lt;DocumentId&gt;(id_val.clone()) {&#xa;                                // ────────────────────────────────────────────────────────&#xa;                                // KEY INSIGHT: HashMap automatically keeps latest version!&#xa;                                // If doc_id already exists, this overwrites the old value.&#xa;                                // ────────────────────────────────────────────────────────&#xa;                                docs_by_id.insert(doc_id, doc);&#xa;                                chunk_count += 1;&#xa;                                &#xa;                                // Memory tracking (for stats)&#xa;                                let doc_size_bytes = doc_bytes.len() as u64;&#xa;                                let current_memory_bytes = docs_by_id.len() as u64 * doc_size_bytes;&#xa;                                let current_memory_mb = current_memory_bytes / (1024 * 1024);&#xa;                                if current_memory_mb &gt; stats.peak_memory_mb {&#xa;                                    stats.peak_memory_mb = current_memory_mb;&#xa;                                }&#xa;                                &#xa;                                // ────────────────────────────────────────────────────────&#xa;                                // CHUNK FLUSH: If chunk is full, flush all collections&#xa;                                // ────────────────────────────────────────────────────────&#xa;                                if chunk_count &gt;= config.chunk_size { // Default: 1000&#xa;                                    for (coll_name, docs) in collection_docs.iter_mut() {&#xa;                                        if !docs.is_empty() {&#xa;                                            write_offset = self.flush_compaction_chunk(&#xa;                                                &amp;mut new_file,&#xa;                                                &amp;mut new_collections,&#xa;                                                coll_name,&#xa;                                                docs,&#xa;                                                write_offset,&#xa;                                                &amp;mut stats,&#xa;                                            )?;&#xa;                                            // CRITICAL: Clear chunk to free memory!&#xa;                                            docs.clear();&#xa;                                        }&#xa;                                    }&#xa;                                    chunk_count = 0; // Reset chunk counter&#xa;                                }&#xa;                            }&#xa;                        }&#xa;                    }&#xa;                }&#xa;                &#xa;                // Advance to next document&#xa;                current_offset += 4 + doc_bytes.len() as u64;&#xa;            }&#xa;            Err(_) =&gt; break, // End of documents or corruption&#xa;        }&#xa;    }&#xa;    &#xa;    // ─────────────────────────────────────────────────────────────────────────&#xa;    // Flush Remaining Documents (partial last chunk)&#xa;    // ─────────────────────────────────────────────────────────────────────────&#xa;    for (coll_name, docs) in collection_docs.iter_mut() {&#xa;        if !docs.is_empty() {&#xa;            write_offset = self.flush_compaction_chunk(&#xa;                &amp;mut new_file, &amp;mut new_collections,&#xa;                coll_name, docs, write_offset, &amp;mut stats,&#xa;            )?;&#xa;        }&#xa;    }&#xa;    &#xa;    new_file.sync_all()?;&#xa;    &#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    // PHASE 4: Write Metadata at END (Version 2 Format)&#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    &#xa;    let metadata_offset = write_offset; // After last document&#xa;    &#xa;    // Serialize metadata&#xa;    let mut metadata_buffer = std::io::Cursor::new(Vec::new());&#xa;    let count = (new_collections.len() as u32).to_le_bytes();&#xa;    metadata_buffer.write_all(&amp;count)?;&#xa;    for meta in new_collections.values() {&#xa;        let meta_bytes = serde_json::to_vec(meta)?;&#xa;        let len = (meta_bytes.len() as u32).to_le_bytes();&#xa;        metadata_buffer.write_all(&amp;len)?;&#xa;        metadata_buffer.write_all(&amp;meta_bytes)?;&#xa;    }&#xa;    let metadata_bytes = metadata_buffer.into_inner();&#xa;    let metadata_size = metadata_bytes.len() as u64;&#xa;    &#xa;    // Write metadata at end&#xa;    new_file.seek(SeekFrom::Start(metadata_offset))?;&#xa;    new_file.write_all(&amp;metadata_bytes)?;&#xa;    &#xa;    // Update header with metadata location&#xa;    let mut updated_header = self.header.clone();&#xa;    updated_header.metadata_offset = metadata_offset;&#xa;    updated_header.metadata_size = metadata_size;&#xa;    &#xa;    // Rewrite header&#xa;    new_file.seek(SeekFrom::Start(0))?;&#xa;    let header_bytes = bincode::serialize(&amp;updated_header)?;&#xa;    new_file.write_all(&amp;header_bytes)?;&#xa;    new_file.sync_all()?;&#xa;    &#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    // PHASE 5: Replace Old File with Compacted File&#xa;    // ══════════════════════════════════════════════════════════════════════════&#xa;    &#xa;    stats.size_after = new_file.metadata()?.len();&#xa;    &#xa;    drop(new_file); // Close temp file&#xa;    drop(self.mmap.take()); // Unmap old file&#xa;    &#xa;    // Atomic rename (on Unix, at least)&#xa;    fs::rename(&amp;temp_path, &amp;self.file_path)?;&#xa;    &#xa;    // Reload compacted database&#xa;    let mut file = OpenOptions::new().read(true).write(true).open(&amp;self.file_path)?;&#xa;    let (header, collections) = Self::load_metadata(&amp;mut file)?;&#xa;    &#xa;    self.file = file;&#xa;    self.header = header;&#xa;    self.collections = collections;&#xa;    self.mmap = None; // Will re-mmap on next read if &lt;1GB&#xa;    &#xa;    Ok(stats)&#xa;}&#xa;&#xa;// ═════════════════════════════════════════════════════════════════════════════&#xa;// PERFORMANCE PROFILE (650K documents, 564 MB file)&#xa;// ═════════════════════════════════════════════════════════════════════════════&#xa;//&#xa;// Phase 1: Flush metadata             ~100-450 ms&#xa;// Phase 2: Create temp file            ~10 ms&#xa;// Phase 3: Scan &amp; process docs         ~30-60 seconds&#xa;//   - Read: 564 MB at ~50 MB/s         ~11 seconds&#xa;//   - Deserialize: 650K * 10 μs        ~6.5 seconds&#xa;//   - HashMap ops: 650K * 50 ns        ~33 ms&#xa;//   - Chunk flushes: 650 * 5 ms        ~3.25 seconds&#xa;// Phase 4: Write metadata              ~50-200 ms&#xa;// Phase 5: Rename &amp; reload             ~100 ms&#xa;// ───────────────────────────────────────────────────────────────────────────&#xa;// TOTAL:                               ~30-70 seconds&#xa;//&#xa;// Memory Usage:&#xa;//   - Peak: ~1000 docs * 100 bytes = 100 KB (constant!)&#xa;//   - HashMap overhead: ~40 bytes/entry * 1000 = 40 KB&#xa;//   - Total: ~140 KB per chunk (vs 65 MB without chunking!)&#xa;// ═════════════════════════════════════════════════════════════════════════════&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=7;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="1630" y="1990" width="1530" height="990" as="geometry" />
        </mxCell>

        <!-- Find Algorithm -->
        <mxCell id="algo_find" value="Algorithm: find(filter) - Query Execution with Filtering" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;fontSize=14;fontStyle=1;align=left;spacingLeft=15;verticalAlign=top;" vertex="1" parent="1">
          <mxGeometry x="3210" y="1965" width="1510" height="1030" as="geometry" />
        </mxCell>

        <mxCell id="algo_find_code" value="&#xa;pub fn find(&amp;self, filter: Value) -&gt; Result&lt;Vec&lt;Value&gt;&gt; {&#xa;    // ┌──────────────────────────────────────────────────────────────────────┐&#xa;    // │ ALGORITHM: Full Collection Scan with Filtering                      │&#xa;    // │                                                                      │&#xa;    // │ Current: O(n) full scan (no indexes yet)                             │&#xa;    // │ Future: O(log n) with B+ tree indexes                                │&#xa;    // │                                                                      │&#xa;    // │ Steps:                                                               │&#xa;    // │   1. Acquire read lock (allows concurrent reads)                     │&#xa;    // │   2. Iterate through document catalog                                │&#xa;    // │   3. For each document:                                              │&#xa;    // │        a. Read from file (mmap or file I/O)                          │&#xa;    // │        b. Deserialize JSON                                           │&#xa;    // │        c. Check tombstone flag (skip if deleted)                     │&#xa;    // │        d. Apply filter (matches_filter)                              │&#xa;    // │        e. If match: add to results                                   │&#xa;    // │   4. Release read lock                                               │&#xa;    // │   5. Return results                                                  │&#xa;    // │                                                                      │&#xa;    // │ Time Complexity: O(n * m)                                            │&#xa;    // │   n = document count                                                 │&#xa;    // │   m = average document size (for deserialization)                    │&#xa;    // │ Space Complexity: O(r * m)                                           │&#xa;    // │   r = result count                                                   │&#xa;    // └──────────────────────────────────────────────────────────────────────┘&#xa;    &#xa;    // ═══════════════════════════════════════════════════════════════════════&#xa;    // PHASE 1: Acquire Read Lock&#xa;    // ═══════════════════════════════════════════════════════════════════════&#xa;    &#xa;    let storage = self.storage.read();&#xa;    // RwLock::read() - blocks only if there's an active writer&#xa;    // Multiple readers can hold read locks simultaneously&#xa;    // Cost: ~10-100 ns (uncontended), ~1-10 μs (contended)&#xa;    &#xa;    // Get collection metadata (contains catalog)&#xa;    let meta = storage.get_collection_meta(&amp;self.name)?;&#xa;    // Cost: HashMap lookup, O(1), ~10 ns&#xa;    &#xa;    let mut results = Vec::new();&#xa;    // Pre-allocate if we know expected result size (future optimization)&#xa;    &#xa;    // ═══════════════════════════════════════════════════════════════════════&#xa;    // PHASE 2: Iterate Through Document Catalog&#xa;    // ═══════════════════════════════════════════════════════════════════════&#xa;    &#xa;    for (&amp;offset) in meta.document_catalog.values() {&#xa;        // Catalog: HashMap&lt;DocumentId, u64 offset&gt;&#xa;        // Example entries:&#xa;        //   DocumentId::Int(1) → 256&#xa;        //   DocumentId::Int(2) → 316&#xa;        //   ...&#xa;        &#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        // Step 2a: Read Document Data from File&#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        let doc_bytes = storage.read_data(offset)?;&#xa;        // read_data() implementation:&#xa;        //   if mmap is available (&lt;1GB file):&#xa;        //     1. offset_usize = offset as usize (256)&#xa;        //     2. len_bytes = &amp;mmap[offset..offset+4]&#xa;        //     3. len = u32::from_le_bytes(len_bytes) (56)&#xa;        //     4. data = &amp;mmap[offset+4..offset+4+len]&#xa;        //     5. return data.to_vec()&#xa;        //     Cost: ~50-200 ns (cache hit), ~1-10 μs (page fault)&#xa;        //   else:&#xa;        //     1. file.seek(SeekFrom::Start(offset))&#xa;        //     2. file.read_exact(&amp;mut len_bytes[0..4])&#xa;        //     3. len = u32::from_le_bytes(len_bytes)&#xa;        //     4. file.read_exact(&amp;mut data[0..len])&#xa;        //     5. return data&#xa;        //     Cost: ~1-10 μs (syscall overhead)&#xa;        &#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        // Step 2b: Deserialize JSON to Value&#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        let doc: Value = serde_json::from_slice(&amp;doc_bytes)?;&#xa;        // serde_json parsing:&#xa;        //   1. UTF-8 validation&#xa;        //   2. Lexical analysis (tokenize)&#xa;        //   3. Syntax parsing (recursive descent)&#xa;        //   4. Allocate Value tree (heap allocations)&#xa;        // Cost: O(m) where m = doc size, ~500 ns - 5 μs per doc&#xa;        &#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        // Step 2c: Check Tombstone (Deleted Documents)&#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        if doc.get(&quot;_tombstone&quot;).and_then(|v| v.as_bool()).unwrap_or(false) {&#xa;            // Document is marked as deleted, skip it&#xa;            // Tombstone pattern: {&quot;_id&quot;: 1, &quot;_tombstone&quot;: true, ...}&#xa;            continue;&#xa;        }&#xa;        // Cost: 2 HashMap lookups + bool check, ~20 ns&#xa;        &#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        // Step 2d: Apply Filter (Query Matching)&#xa;        // ───────────────────────────────────────────────────────────────────&#xa;        if matches_filter(&amp;doc, &amp;filter)? {&#xa;            // matches_filter() detailed implementation below&#xa;            // Cost: O(f) where f = filter complexity&#xa;            //   Simple equality: ~50 ns&#xa;            //   Range query ($gt, $lt): ~100 ns&#xa;            //   Complex ($and, $or): ~500 ns - 5 μs&#xa;            &#xa;            // ───────────────────────────────────────────────────────────────&#xa;            // Step 2e: Add to Results&#xa;            // ───────────────────────────────────────────────────────────────&#xa;            results.push(doc);&#xa;            // Cost: O(1) amortized (Vec push)&#xa;            // If Vec needs to grow: O(current_size) realloc + copy&#xa;        }&#xa;    }&#xa;    &#xa;    // ═══════════════════════════════════════════════════════════════════════&#xa;    // PHASE 3: Return Results&#xa;    // ═══════════════════════════════════════════════════════════════════════&#xa;    &#xa;    Ok(results)&#xa;    // Read lock automatically released when `storage` goes out of scope&#xa;}&#xa;&#xa;// ───────────────────────────────────────────────────────────────────────────&#xa;// matches_filter() - Detailed Filter Matching Logic&#xa;// ───────────────────────────────────────────────────────────────────────────&#xa;pub fn matches_filter(doc: &amp;Value, filter: &amp;Value) -&gt; Result&lt;bool&gt; {&#xa;    // Empty filter {} matches all documents&#xa;    if filter.as_object().map(|o| o.is_empty()).unwrap_or(false) {&#xa;        return Ok(true);&#xa;    }&#xa;    &#xa;    let filter_obj = filter.as_object().ok_or(QueryError)?;&#xa;    &#xa;    // Iterate through filter conditions&#xa;    for (key, value) in filter_obj {&#xa;        match key.as_str() {&#xa;            // ═══════════════════════════════════════════════════════════════&#xa;            // Logical Operators&#xa;            // ═══════════════════════════════════════════════════════════════&#xa;            &quot;$and&quot; =&gt; {&#xa;                // Example: {&quot;$and&quot;: [{&quot;age&quot;: {&quot;$gt&quot;: 18}}, {&quot;age&quot;: {&quot;$lt&quot;: 65}}]}&#xa;                let conditions = value.as_array().ok_or(QueryError)?;&#xa;                for cond in conditions {&#xa;                    if !matches_filter(doc, cond)? {&#xa;                        return Ok(false); // Short-circuit: any false = false&#xa;                    }&#xa;                }&#xa;            }&#xa;            &quot;$or&quot; =&gt; {&#xa;                // Example: {&quot;$or&quot;: [{&quot;age&quot;: {&quot;$lt&quot;: 18}}, {&quot;age&quot;: {&quot;$gt&quot;: 65}}]}&#xa;                let conditions = value.as_array().ok_or(QueryError)?;&#xa;                let mut any_match = false;&#xa;                for cond in conditions {&#xa;                    if matches_filter(doc, cond)? {&#xa;                        any_match = true;&#xa;                        break; // Short-circuit: any true = true&#xa;                    }&#xa;                }&#xa;                if !any_match {&#xa;                    return Ok(false);&#xa;                }&#xa;            }&#xa;            &#xa;            // ═══════════════════════════════════════════════════════════════&#xa;            // Field-Level Operators&#xa;            // ═══════════════════════════════════════════════════════════════&#xa;            field_name =&gt; {&#xa;                // Get field value from document&#xa;                let doc_value = doc.get(field_name);&#xa;                &#xa;                if let Some(operator_obj) = value.as_object() {&#xa;                    // Operator query: {&quot;age&quot;: {&quot;$gt&quot;: 18}}&#xa;                    for (op, op_value) in operator_obj {&#xa;                        match op.as_str() {&#xa;                            &quot;$eq&quot; =&gt; {&#xa;                                // Equality: {&quot;name&quot;: {&quot;$eq&quot;: &quot;Alice&quot;}}&#xa;                                if doc_value != Some(op_value) {&#xa;                                    return Ok(false);&#xa;                                }&#xa;                            }&#xa;                            &quot;$ne&quot; =&gt; {&#xa;                                // Not equal: {&quot;age&quot;: {&quot;$ne&quot;: 30}}&#xa;                                if doc_value == Some(op_value) {&#xa;                                    return Ok(false);&#xa;                                }&#xa;                            }&#xa;                            &quot;$gt&quot; =&gt; {&#xa;                                // Greater than: {&quot;age&quot;: {&quot;$gt&quot;: 18}}&#xa;                                if !compare_gt(doc_value, op_value)? {&#xa;                                    return Ok(false);&#xa;                                }&#xa;                            }&#xa;                            &quot;$gte&quot; | &quot;$lt&quot; | &quot;$lte&quot; =&gt; { /* similar */ }&#xa;                            &quot;$in&quot; =&gt; {&#xa;                                // In array: {&quot;status&quot;: {&quot;$in&quot;: [&quot;active&quot;, &quot;pending&quot;]}}&#xa;                                let arr = op_value.as_array().ok_or(QueryError)?;&#xa;                                let doc_val = doc_value.unwrap_or(&amp;Value::Null);&#xa;                                if !arr.contains(doc_val) {&#xa;                                    return Ok(false);&#xa;                                }&#xa;                            }&#xa;                            &quot;$exists&quot; =&gt; {&#xa;                                // Field exists: {&quot;email&quot;: {&quot;$exists&quot;: true}}&#xa;                                let should_exist = op_value.as_bool().ok_or(QueryError)?;&#xa;                                if should_exist != doc_value.is_some() {&#xa;                                    return Ok(false);&#xa;                                }&#xa;                            }&#xa;                            _ =&gt; return Err(MongoLiteError::UnsupportedOperator(op.clone())),&#xa;                        }&#xa;                    }&#xa;                } else {&#xa;                    // Simple equality: {&quot;name&quot;: &quot;Alice&quot;}&#xa;                    if doc_value != Some(value) {&#xa;                        return Ok(false);&#xa;                    }&#xa;                }&#xa;            }&#xa;        }&#xa;    }&#xa;    &#xa;    Ok(true) // All conditions passed&#xa;}&#xa;&#xa;// ═════════════════════════════════════════════════════════════════════════════&#xa;// PERFORMANCE PROFILE&#xa;// ═════════════════════════════════════════════════════════════════════════════&#xa;//&#xa;// Small Collection (100 docs, 10 KB each):&#xa;//   - Catalog iteration: 100 * 10 ns = 1 μs&#xa;//   - Read (mmap): 100 * 200 ns = 20 μs&#xa;//   - Deserialize: 100 * 2 μs = 200 μs&#xa;//   - Filter: 100 * 100 ns = 10 μs&#xa;//   TOTAL: ~231 μs (~0.23 ms)&#xa;//&#xa;// Medium Collection (10K docs, 100 bytes each):&#xa;//   - Catalog iteration: 10K * 10 ns = 100 μs&#xa;//   - Read (mmap): 10K * 200 ns = 2 ms&#xa;//   - Deserialize: 10K * 1 μs = 10 ms&#xa;//   - Filter: 10K * 100 ns = 1 ms&#xa;//   TOTAL: ~13 ms&#xa;//&#xa;// Large Collection (50K docs, 100 bytes each):&#xa;//   - Catalog iteration: 50K * 10 ns = 500 μs&#xa;//   - Read (mmap): 50K * 200 ns = 10 ms&#xa;//   - Deserialize: 50K * 1 μs = 50 ms&#xa;//   - Filter: 50K * 100 ns = 5 ms&#xa;//   TOTAL: ~65 ms&#xa;//   Observed: 0.29s for 7143 results (electronics category)&#xa;//   Difference: Python overhead + PyO3 conversion + result collection&#xa;//&#xa;// Very Large Collection (650K docs):&#xa;//   - Estimated: ~800-1000 ms&#xa;//   - Observed: Deserialization bug (separate issue)&#xa;// ═════════════════════════════════════════════════════════════════════════════&#xa;" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=7;fontFamily=Courier New;spacingLeft=10;" vertex="1" parent="1">
          <mxGeometry x="3220" y="1990" width="1490" height="990" as="geometry" />
        </mxCell>

        <!-- Final Note -->
        <mxCell id="final_note" value="COMPREHENSIVE ARCHITECTURE DOCUMENTATION COMPLETE" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#000000;strokeColor=#000000;fontColor=#FFFFFF;fontSize=24;fontStyle=1;align=center;" vertex="1" parent="1">
          <mxGeometry x="40" y="3060" width="4720" height="80" as="geometry" />
        </mxCell>

        <mxCell id="final_text" value="This diagram contains COMPLETE implementation details including:\n• Memory layouts with byte-level precision\n• Complete file format with hex dumps and examples\n• Full algorithm pseudocode with complexity analysis\n• Performance profiling data for all operations\n• Detailed data flow from Python through Rust to storage\n\nOpen in DrawIO (draw.io) for full exploration!" style="text;html=1;strokeColor=none;fillColor=#333333;fontColor=#FFFFFF;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=14;" vertex="1" parent="1">
          <mxGeometry x="1400" y="3160" width="2000" height="120" as="geometry" />
        </mxCell>

      </root>
    </mxGraphModel>
  </diagram>
</mxfile>
