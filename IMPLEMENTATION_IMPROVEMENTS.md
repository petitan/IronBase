# IMPLEMENTATION_IMPROVEMENTS.md
# IronBase - Comprehensive Development Roadmap & Implementation Guide

**Status:** Draft
**Created:** 2025-11-11
**Based on:** Engineering Code Analysis Report
**Overall Score:** 8.5/10 â­â­â­â­â­â­â­â­

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Phase 1: Critical Fixes (1-2 weeks)](#phase-1-critical-fixes)
3. [Phase 2: Performance Improvements (2-3 weeks)](#phase-2-performance-improvements)
4. [Phase 3: Feature Completeness (3-4 weeks)](#phase-3-feature-completeness)
5. [Phase 4: Scalability (4-6 weeks)](#phase-4-scalability)
6. [Testing Strategy](#testing-strategy)
7. [Migration Guide](#migration-guide)
8. [Appendix: Trade-offs & Alternatives](#appendix)

---

## Executive Summary

### Current State

IronBase is a **production-ready embedded database** for light-to-medium workloads (< 1M docs, < 100 concurrent users). The codebase demonstrates excellent architecture with several standout optimizations:

**Strengths:**
- âœ… Document Catalog Optimization (O(1) lookups, 1000x speedup)
- âœ… Reserved Metadata Architecture (prevents corruption)
- âœ… WAL-based ACD Transactions (proper durability)
- âœ… Query Cache (LRU, 10-100x speedup)
- âœ… Clean Rust code (idiomatic, well-tested)

**Critical Weaknesses:**
- âš ï¸ **Index Non-Atomicity** - Crash can cause data/index inconsistency
- âš ï¸ **Compaction Memory Bottleneck** - Entire DB loaded into RAM
- âš ï¸ **Write Lock Contention** - Writes block all reads

### Roadmap Overview

| Phase | Focus | Duration | Priority |
|-------|-------|----------|----------|
| **Phase 1** | Critical Fixes | 1-2 weeks | ğŸš¨ HIGH |
| **Phase 2** | Performance | 2-3 weeks | âš¡ MEDIUM |
| **Phase 3** | Features | 3-4 weeks | ğŸ’¡ MEDIUM |
| **Phase 4** | Scalability | 4-6 weeks | ğŸš€ LOW |

**Total Estimated Time:** 8-16 weeks (2-4 months)

### Impact Forecast

After completing all phases:
- **Reliability:** 7/10 â†’ **10/10** (index atomicity fixed)
- **Performance:** 8/10 â†’ **9.5/10** (MVCC, streaming compaction)
- **Scalability:** 6/10 â†’ **9/10** (multi-file, 10M+ docs support)
- **Features:** 7/10 â†’ **9/10** (MongoDB near-parity)

---

## Phase 1: Critical Fixes

**Goal:** Ensure data integrity and prevent OOM crashes
**Duration:** 1-2 weeks
**Priority:** ğŸš¨ CRITICAL

### 1.1 Index Atomicity Fix (Two-Phase Commit)

#### Problem Statement

**Current Issue:**
```rust
// collection_core.rs:1079-1110
pub fn insert_one_tx(&self, doc: HashMap<String, Value>, tx: &mut Transaction) -> Result<DocumentId> {
    tx.add_operation(Operation::Insert { ... })?;

    // TODO: Track index changes (future: two-phase commit)
    // ^^^^^ INDEX UPDATES NOT TRANSACTIONAL!
}
```

**Scenario:**
1. Transaction commits data to WAL âœ…
2. Data written to storage âœ…
3. **CRASH HAPPENS HERE** ğŸ’¥
4. Index update never completes âŒ

**Consequence:**
- Index points to wrong/missing documents
- Query returns incorrect results
- Data integrity violated

#### Solution Design: Two-Phase Commit Protocol

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transaction Commit with Indexes (2PC)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 1: PREPARE                                       â”‚
â”‚  â”œâ”€ Write operations to WAL                  âœ“ Durable â”‚
â”‚  â”œâ”€ Create temp index files (.tmp)           âœ“ Atomic  â”‚
â”‚  â””â”€ Fsync WAL                                âœ“ Persist â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 2: COMMIT                                        â”‚
â”‚  â”œâ”€ Apply data changes to storage            âœ“ Durable â”‚
â”‚  â”œâ”€ Atomic rename: .tmp â†’ .idx               âœ“ Atomic  â”‚
â”‚  â”œâ”€ Fsync storage file                       âœ“ Persist â”‚
â”‚  â””â”€ Write COMMIT marker to WAL               âœ“ Final   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Pseudocode Implementation

**Step 1: Define IndexChange Tracking**

```rust
// ironbase-core/src/transaction.rs

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum IndexChange {
    Insert {
        index_name: String,
        key: IndexKey,
        doc_id: DocumentId,
    },
    Delete {
        index_name: String,
        key: IndexKey,
        doc_id: DocumentId,
    },
    Update {
        index_name: String,
        old_key: IndexKey,
        new_key: IndexKey,
        doc_id: DocumentId,
    },
}

pub struct Transaction {
    pub id: u64,
    operations: Vec<Operation>,
    index_changes: Vec<IndexChange>,  // NEW!
    metadata_changes: Vec<MetadataChange>,
    status: TransactionStatus,
}

impl Transaction {
    pub fn add_index_change(&mut self, change: IndexChange) {
        self.index_changes.push(change);
    }

    pub fn index_changes(&self) -> &[IndexChange] {
        &self.index_changes
    }
}
```

**Step 2: Prepare Phase - Create Temp Index Files**

```rust
// ironbase-core/src/index.rs

impl BPlusTree {
    /// Phase 1: Prepare index changes (write to temp file)
    pub fn prepare_changes(&mut self, changes: &[IndexChange]) -> Result<PathBuf> {
        let temp_path = format!("{}.tmp", self.metadata.file_path);
        let mut temp_file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .truncate(true)
            .open(&temp_path)?;

        // Clone current index structure
        let mut staging_tree = self.clone();

        // Apply all changes to staging tree
        for change in changes {
            match change {
                IndexChange::Insert { key, doc_id, .. } => {
                    staging_tree.insert(key.clone(), doc_id.clone())?;
                }
                IndexChange::Delete { key, doc_id, .. } => {
                    staging_tree.delete(key, doc_id)?;
                }
                IndexChange::Update { old_key, new_key, doc_id, .. } => {
                    staging_tree.delete(old_key, doc_id)?;
                    staging_tree.insert(new_key.clone(), doc_id.clone())?;
                }
            }
        }

        // Serialize staging tree to temp file
        staging_tree.save_to_file(&mut temp_file)?;

        // Fsync temp file (ensure it's durable)
        temp_file.sync_all()?;

        Ok(PathBuf::from(temp_path))
    }
}
```

**Step 3: Commit Phase - Atomic Rename**

```rust
// ironbase-core/src/index.rs

impl BPlusTree {
    /// Phase 2: Commit index changes (atomic rename)
    pub fn commit_prepared_changes(&mut self, temp_path: &Path) -> Result<()> {
        let final_path = &self.metadata.file_path;

        // Atomic rename (POSIX guarantee)
        fs::rename(temp_path, final_path)?;

        // Reload index from new file
        self.reload_from_file()?;

        Ok(())
    }

    /// Rollback prepared changes (delete temp file)
    pub fn rollback_prepared_changes(temp_path: &Path) -> Result<()> {
        if temp_path.exists() {
            fs::remove_file(temp_path)?;
        }
        Ok(())
    }
}
```

**Step 4: Integrate into Transaction Commit**

```rust
// ironbase-core/src/storage/mod.rs

impl StorageEngine {
    pub fn commit_transaction(&mut self, transaction: &mut Transaction) -> Result<()> {
        // ===== PHASE 1: PREPARE =====

        // Step 1: Write BEGIN marker to WAL
        self.wal.append(&WALEntry::new(transaction.id, WALEntryType::Begin, vec![]))?;

        // Step 2: Write all data operations to WAL
        for operation in transaction.operations() {
            let op_json = serde_json::to_string(operation)?;
            self.wal.append(&WALEntry::new(
                transaction.id,
                WALEntryType::Operation,
                op_json.as_bytes().to_vec()
            ))?;
        }

        // Step 3: Write all index changes to WAL
        for index_change in transaction.index_changes() {
            let change_json = serde_json::to_string(index_change)?;
            self.wal.append(&WALEntry::new(
                transaction.id,
                WALEntryType::IndexChange,  // NEW entry type!
                change_json.as_bytes().to_vec()
            ))?;
        }

        // Step 4: Write COMMIT marker to WAL
        self.wal.append(&WALEntry::new(transaction.id, WALEntryType::Commit, vec![]))?;

        // Step 5: Fsync WAL (DURABILITY POINT!)
        self.wal.flush()?;

        // Step 6: Prepare index changes (create temp files)
        let mut temp_index_files: Vec<(String, PathBuf)> = Vec::new();
        for (index_name, changes) in Self::group_index_changes(transaction.index_changes()) {
            if let Some(index) = self.get_index_mut(&index_name) {
                let temp_path = index.prepare_changes(&changes)?;
                temp_index_files.push((index_name.clone(), temp_path));
            }
        }

        // ===== PHASE 2: COMMIT =====

        // Step 7: Apply data operations to storage
        self.apply_operations(transaction)?;

        // Step 8: Fsync storage file
        self.file.sync_all()?;

        // Step 9: Commit index changes (atomic renames)
        for (index_name, temp_path) in temp_index_files {
            if let Some(index) = self.get_index_mut(&index_name) {
                index.commit_prepared_changes(&temp_path)?;
            }
        }

        // Step 10: Apply metadata changes
        for metadata_change in transaction.metadata_changes() {
            if let Some(meta) = self.collections.get_mut(&metadata_change.collection) {
                meta.last_id = metadata_change.last_id as u64;
                meta.document_count = metadata_change.document_count;
            }
        }

        // Step 11: Flush metadata
        self.flush_metadata()?;

        // Step 12: Mark transaction as committed
        transaction.mark_committed()?;

        Ok(())
    }

    fn group_index_changes(changes: &[IndexChange]) -> HashMap<String, Vec<IndexChange>> {
        let mut grouped: HashMap<String, Vec<IndexChange>> = HashMap::new();
        for change in changes {
            let index_name = match change {
                IndexChange::Insert { index_name, .. } => index_name,
                IndexChange::Delete { index_name, .. } => index_name,
                IndexChange::Update { index_name, .. } => index_name,
            };
            grouped.entry(index_name.clone())
                .or_insert_with(Vec::new)
                .push(change.clone());
        }
        grouped
    }
}
```

**Step 5: Crash Recovery**

```rust
// ironbase-core/src/wal.rs

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum WALEntryType {
    Begin,
    Operation,
    IndexChange,  // NEW!
    Commit,
    Abort,
}

// ironbase-core/src/storage/mod.rs

impl StorageEngine {
    pub fn recover_from_wal(&mut self) -> Result<()> {
        let recovered = self.wal.recover()?;  // Returns only COMMITTED transactions

        for tx_entries in recovered {
            let mut data_operations = Vec::new();
            let mut index_changes = Vec::new();

            // Separate data operations and index changes
            for entry in tx_entries {
                match entry.entry_type {
                    WALEntryType::Operation => {
                        let operation: Operation = serde_json::from_slice(&entry.data)?;
                        data_operations.push(operation);
                    }
                    WALEntryType::IndexChange => {
                        let change: IndexChange = serde_json::from_slice(&entry.data)?;
                        index_changes.push(change);
                    }
                    _ => {}
                }
            }

            // Replay data operations
            for operation in data_operations {
                self.apply_operation(&operation)?;
            }

            // Replay index changes
            for (index_name, changes) in Self::group_index_changes(&index_changes) {
                if let Some(index) = self.get_index_mut(&index_name) {
                    // Apply changes directly (no temp file needed in recovery)
                    for change in changes {
                        match change {
                            IndexChange::Insert { key, doc_id, .. } => {
                                index.insert(key, doc_id)?;
                            }
                            IndexChange::Delete { key, doc_id, .. } => {
                                index.delete(&key, &doc_id)?;
                            }
                            IndexChange::Update { old_key, new_key, doc_id, .. } => {
                                index.delete(&old_key, &doc_id)?;
                                index.insert(new_key, doc_id)?;
                            }
                        }
                    }

                    // Persist updated index
                    index.save()?;
                }
            }
        }

        // Clear WAL after successful recovery
        self.wal.clear()?;

        Ok(())
    }
}
```

**Step 6: Update Collection Methods to Track Index Changes**

```rust
// ironbase-core/src/collection_core.rs

impl CollectionCore {
    pub fn insert_one_tx(&self, mut fields: HashMap<String, Value>, tx: &mut Transaction) -> Result<DocumentId> {
        // ... existing code for document insertion ...

        // Track index changes
        {
            let indexes = self.indexes.read();
            for index_meta in indexes.list() {
                if let Some(field_value) = fields.get(&index_meta.field) {
                    let index_key = IndexKey::from_value(field_value)?;

                    tx.add_index_change(IndexChange::Insert {
                        index_name: index_meta.name.clone(),
                        key: index_key,
                        doc_id: doc_id.clone(),
                    });
                }
            }
        }

        Ok(doc_id)
    }

    pub fn update_one_tx(&self, query_json: &Value, update_json: &Value, tx: &mut Transaction) -> Result<u64> {
        // ... find document ...

        // Track index changes for updated fields
        {
            let indexes = self.indexes.read();
            for index_meta in indexes.list() {
                let field = &index_meta.field;

                let old_value = old_doc.get(field);
                let new_value = new_doc.get(field);

                // If indexed field changed, track update
                if old_value != new_value {
                    if let (Some(old_val), Some(new_val)) = (old_value, new_value) {
                        tx.add_index_change(IndexChange::Update {
                            index_name: index_meta.name.clone(),
                            old_key: IndexKey::from_value(old_val)?,
                            new_key: IndexKey::from_value(new_val)?,
                            doc_id: doc_id.clone(),
                        });
                    }
                }
            }
        }

        Ok(modified_count)
    }

    pub fn delete_one_tx(&self, query_json: &Value, tx: &mut Transaction) -> Result<u64> {
        // ... find document ...

        // Track index deletions
        {
            let indexes = self.indexes.read();
            for index_meta in indexes.list() {
                if let Some(field_value) = doc.get(&index_meta.field) {
                    tx.add_index_change(IndexChange::Delete {
                        index_name: index_meta.name.clone(),
                        key: IndexKey::from_value(field_value)?,
                        doc_id: doc_id.clone(),
                    });
                }
            }
        }

        Ok(deleted_count)
    }
}
```

#### Implementation Steps

1. **Day 1-2:** Add `IndexChange` enum and tracking to `Transaction` struct
2. **Day 3-4:** Implement `prepare_changes()` and `commit_prepared_changes()` in `BPlusTree`
3. **Day 5-6:** Update `commit_transaction()` to use two-phase commit
4. **Day 7-8:** Implement WAL recovery with index changes
5. **Day 9-10:** Update `insert_one_tx`, `update_one_tx`, `delete_one_tx` to track index changes

#### Testing Strategy

**Unit Tests:**
```rust
#[test]
fn test_index_atomicity_commit() {
    let db = create_test_db();
    let mut tx = Transaction::new(1);

    // Insert document with indexed field
    db.collection("users").insert_one_tx(
        hashmap!{ "age" => 25 },
        &mut tx
    ).unwrap();

    // Commit transaction
    db.commit_transaction(&mut tx).unwrap();

    // Verify index was updated
    let index = db.get_index("users_age").unwrap();
    assert!(index.contains_key(&IndexKey::Int(25)));
}

#[test]
fn test_index_atomicity_crash_recovery() {
    let db_path = "test_crash.mlite";

    {
        let db = IronBase::open(db_path).unwrap();
        db.collection("users").create_index("age", false).unwrap();

        let mut tx = Transaction::new(1);
        db.collection("users").insert_one_tx(
            hashmap!{ "age" => 30 },
            &mut tx
        ).unwrap();

        // Simulate crash AFTER WAL commit but BEFORE index commit
        // (manually test by adding panic! before index.commit_prepared_changes())
        db.commit_transaction(&mut tx).unwrap();
    }

    // Reopen database (triggers WAL recovery)
    {
        let db = IronBase::open(db_path).unwrap();

        // Verify index was recovered
        let results = db.collection("users").find(&json!({"age": 30})).unwrap();
        assert_eq!(results.len(), 1);

        // Verify index consistency
        let index = db.get_index("users_age").unwrap();
        assert!(index.contains_key(&IndexKey::Int(30)));
    }
}
```

**Integration Tests:**
```rust
#[test]
fn test_concurrent_transactions_with_indexes() {
    let db = Arc::new(IronBase::open("test_concurrent.mlite").unwrap());
    db.collection("users").create_index("email", true).unwrap();  // unique index

    let mut handles = vec![];

    // Spawn 10 concurrent transactions
    for i in 0..10 {
        let db_clone = Arc::clone(&db);
        let handle = thread::spawn(move || {
            let mut tx = Transaction::new(i as u64);
            db_clone.collection("users").insert_one_tx(
                hashmap!{ "email" => format!("user{}@example.com", i) },
                &mut tx
            ).unwrap();
            db_clone.commit_transaction(&mut tx).unwrap();
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }

    // Verify all 10 documents are indexed
    let index = db.get_index("users_email").unwrap();
    assert_eq!(index.len(), 10);
}
```

#### Breaking Changes

**None** - This is a backward-compatible enhancement. Existing databases will work without migration.

**WAL Format Change:**
- New entry type: `WALEntryType::IndexChange`
- Old databases without index changes will still recover correctly (ignored entry types)

#### Performance Impact

**Overhead:**
- Write latency: +5-10% (temp file creation + atomic rename)
- Disk I/O: +1 fsync per transaction (temp index file)
- Memory: Minimal (staging tree is cloned, not entire index)

**Benefits:**
- âœ… **Data integrity guaranteed**
- âœ… **No more index inconsistency after crashes**
- âœ… **Production-ready reliability**

---

### 1.2 Streaming Compaction

#### Problem Statement

**Current Issue:**
```rust
// storage/compaction.rs:46
let mut all_docs: HashMap<String, HashMap<DocumentId, Value>> = HashMap::new();
// ^^^^^ ENTIRE DATABASE LOADED INTO RAM!
```

**Memory Usage:**
- 10K docs @ 1KB/doc = ~10MB RAM (acceptable)
- 100K docs @ 1KB/doc = ~100MB RAM (acceptable)
- 1M docs @ 1KB/doc = **~1GB RAM** (problematic!)
- 10M docs @ 1KB/doc = **~10GB RAM** (OOM crash!)

**Consequence:**
- Large databases cannot be compacted
- OOM crash kills the entire application
- Compaction blocks all operations for minutes

#### Solution Design: Batch-Wise Streaming Compaction

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OLD APPROACH: Load Entire DB into RAM                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  all_docs = HashMap::new()  â† 1GB+ memory!               â”‚
â”‚  for doc in entire_database {                            â”‚
â”‚      all_docs.insert(doc.id, doc);                       â”‚
â”‚  }                                                        â”‚
â”‚  write_to_new_file(all_docs);                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEW APPROACH: Streaming with Fixed-Size Batches         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  batch = Vec::with_capacity(BATCH_SIZE)  â† 32MB fixed    â”‚
â”‚  for chunk in database_chunks() {                        â”‚
â”‚      batch.extend(read_chunk(1000 docs));                â”‚
â”‚      if batch.is_full() {                                â”‚
â”‚          write_batch_to_temp(batch);                     â”‚
â”‚          batch.clear();  â† Memory reused!                â”‚
â”‚      }                                                    â”‚
â”‚  }                                                        â”‚
â”‚  atomic_rename(temp_file, original_file);                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Memory Savings:**
- **Before:** O(n * avg_doc_size) = O(1GB) @ 1M docs
- **After:** O(batch_size * avg_doc_size) = O(32MB) fixed!

#### Pseudocode Implementation

**Step 1: Define Batch Iterator**

```rust
// ironbase-core/src/storage/compaction.rs

const BATCH_SIZE: usize = 1000;  // 1000 documents per batch (~1MB @ 1KB/doc)

pub struct DocumentBatchIterator<'a> {
    storage: &'a StorageEngine,
    collection_name: String,
    current_offset: u64,
    end_offset: u64,
    batch_size: usize,
}

impl<'a> DocumentBatchIterator<'a> {
    pub fn new(storage: &'a StorageEngine, collection_name: &str, batch_size: usize) -> Result<Self> {
        let meta = storage.get_collection_meta(collection_name)?;
        let data_offset = meta.data_offset;
        let file_len = storage.file_len()?;

        Ok(DocumentBatchIterator {
            storage,
            collection_name: collection_name.to_string(),
            current_offset: data_offset,
            end_offset: file_len,
            batch_size,
        })
    }

    /// Read next batch of documents (skip tombstones)
    pub fn next_batch(&mut self) -> Result<Vec<(DocumentId, Value)>> {
        let mut batch = Vec::with_capacity(self.batch_size);

        while self.current_offset < self.end_offset && batch.len() < self.batch_size {
            match self.storage.read_data(self.current_offset) {
                Ok(doc_bytes) => {
                    if let Ok(doc) = serde_json::from_slice::<Value>(&doc_bytes) {
                        // Check if document belongs to this collection
                        if doc.get("_collection").and_then(|v| v.as_str()) == Some(&self.collection_name) {
                            // Skip tombstones
                            if !doc.get("_tombstone").and_then(|v| v.as_bool()).unwrap_or(false) {
                                if let Some(id_val) = doc.get("_id") {
                                    if let Ok(doc_id) = serde_json::from_value::<DocumentId>(id_val.clone()) {
                                        batch.push((doc_id, doc));
                                    }
                                }
                            }
                        }
                    }

                    self.current_offset += 4 + doc_bytes.len() as u64;
                }
                Err(_) => break,
            }
        }

        Ok(batch)
    }

    pub fn has_more(&self) -> bool {
        self.current_offset < self.end_offset
    }
}
```

**Step 2: Streaming Compaction Implementation**

```rust
// ironbase-core/src/storage/compaction.rs

impl StorageEngine {
    /// Streaming compaction - constant memory usage
    pub fn compact_streaming(&mut self) -> Result<CompactionStats> {
        let temp_path = format!("{}.compact", self.file_path);
        let mut stats = CompactionStats::default();

        // Get current file size
        stats.size_before = self.file.metadata()?.len();

        // Create temporary new file
        let mut new_file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .truncate(true)
            .open(&temp_path)?;

        // Reserve metadata space (same as normal storage)
        new_file.seek(SeekFrom::Start(0))?;
        let mut new_collections = self.collections.clone();
        for coll_meta in new_collections.values_mut() {
            coll_meta.data_offset = super::DATA_START_OFFSET;
            coll_meta.document_catalog.clear();
            coll_meta.document_count = 0;
        }
        Self::write_metadata(&mut new_file, &self.header, &new_collections)?;

        // Position at DATA_START_OFFSET to begin writing documents
        new_file.seek(SeekFrom::Start(super::DATA_START_OFFSET))?;
        let mut write_offset = super::DATA_START_OFFSET;

        // Process each collection in batches
        let collections_snapshot: Vec<String> = self.collections.keys().cloned().collect();

        for coll_name in collections_snapshot {
            let mut batch_iter = DocumentBatchIterator::new(self, &coll_name, BATCH_SIZE)?;

            while batch_iter.has_more() {
                let batch = batch_iter.next_batch()?;

                stats.documents_scanned += batch.len() as u64;

                // Write batch to new file
                for (doc_id, doc) in batch {
                    let doc_offset = write_offset;
                    let doc_bytes = serde_json::to_vec(&doc)?;
                    let len = doc_bytes.len() as u32;

                    new_file.write_all(&len.to_le_bytes())?;
                    new_file.write_all(&doc_bytes)?;

                    write_offset += 4 + doc_bytes.len() as u64;
                    stats.documents_kept += 1;

                    // Update document_catalog with actual offset
                    if let Some(coll_meta) = new_collections.get_mut(&coll_name) {
                        coll_meta.document_catalog.insert(doc_id.clone(), doc_offset);
                        coll_meta.document_count += 1;
                    }
                }

                // Batch processed - memory freed automatically when `batch` goes out of scope
            }
        }

        // Calculate tombstones removed
        stats.tombstones_removed = stats.documents_scanned - stats.documents_kept;

        // Fsync new file
        new_file.sync_all()?;

        // Rewrite metadata with populated document_catalog
        new_file.seek(SeekFrom::Start(0))?;
        Self::write_metadata(&mut new_file, &self.header, &new_collections)?;
        new_file.sync_all()?;

        // Get new file size
        stats.size_after = new_file.metadata()?.len();

        // Close new file before renaming
        drop(new_file);

        // Close old file and mmap
        drop(self.mmap.take());

        // Atomic file replacement
        fs::rename(&temp_path, &self.file_path)?;

        // Reopen the compacted file
        let mut file = OpenOptions::new()
            .read(true)
            .write(true)
            .open(&self.file_path)?;

        // Reload metadata
        let (header, collections) = Self::load_metadata(&mut file)?;

        // Update self
        self.file = file;
        self.header = header;
        self.collections = collections;
        self.mmap = None;

        Ok(stats)
    }
}
```

**Step 3: Replace Old Compaction Method**

```rust
// ironbase-core/src/storage/mod.rs

impl StorageEngine {
    /// Public compaction method (now uses streaming)
    pub fn compact(&mut self) -> Result<CompactionStats> {
        // Use streaming compaction by default
        self.compact_streaming()
    }

    /// Legacy compaction (kept for testing/benchmarking)
    #[cfg(test)]
    pub fn compact_legacy(&mut self) -> Result<CompactionStats> {
        // ... old implementation (load entire DB into RAM) ...
    }
}
```

#### Implementation Steps

1. **Day 1:** Implement `DocumentBatchIterator`
2. **Day 2-3:** Implement `compact_streaming()` method
3. **Day 4:** Replace old `compact()` with streaming version
4. **Day 5:** Add progress tracking and cancellation support (optional)

#### Testing Strategy

**Unit Tests:**
```rust
#[test]
fn test_streaming_compaction_small_db() {
    let db = create_test_db();

    // Insert 100 documents
    for i in 0..100 {
        db.collection("users").insert_one(hashmap!{ "id" => i }).unwrap();
    }

    // Delete 50 documents (create tombstones)
    for i in 0..50 {
        db.collection("users").delete_one(&json!({"id": i})).unwrap();
    }

    let stats = db.compact().unwrap();

    assert_eq!(stats.documents_scanned, 100);
    assert_eq!(stats.documents_kept, 50);
    assert_eq!(stats.tombstones_removed, 50);
    assert!(stats.space_saved() > 0);
}

#[test]
fn test_streaming_compaction_large_db() {
    let db = create_test_db();

    // Insert 100,000 documents (100MB+)
    for i in 0..100_000 {
        db.collection("users").insert_one(hashmap!{
            "id" => i,
            "name" => format!("User {}", i),
            "email" => format!("user{}@example.com", i),
            "data" => "x".repeat(1000),  // 1KB per doc
        }).unwrap();
    }

    // Measure memory before compaction
    let mem_before = get_current_memory_usage();

    let stats = db.compact().unwrap();

    // Measure memory after compaction
    let mem_after = get_current_memory_usage();
    let mem_used = mem_after - mem_before;

    // Assert memory usage is bounded (< 100MB)
    assert!(mem_used < 100 * 1024 * 1024, "Memory usage too high: {} bytes", mem_used);

    assert_eq!(stats.documents_kept, 100_000);
}

#[test]
fn test_compaction_preserves_document_catalog() {
    let db = create_test_db();

    // Insert documents
    let ids: Vec<DocumentId> = (0..1000)
        .map(|i| db.collection("users").insert_one(hashmap!{ "id" => i }).unwrap())
        .collect();

    // Compact
    db.compact().unwrap();

    // Verify all documents are still accessible by _id
    for doc_id in ids {
        let result = db.collection("users").find_one(&json!({"_id": doc_id})).unwrap();
        assert!(result.is_some());
    }
}
```

**Benchmark:**
```rust
#[bench]
fn bench_compaction_streaming_vs_legacy(b: &mut Bencher) {
    let db = create_test_db();

    // Insert 50,000 documents
    for i in 0..50_000 {
        db.collection("users").insert_one(hashmap!{ "id" => i }).unwrap();
    }

    // Benchmark streaming compaction
    b.iter(|| {
        db.compact().unwrap();
    });
}
```

#### Breaking Changes

**None** - Streaming compaction is a drop-in replacement.

#### Performance Impact

**Speed:**
- **Small DBs (< 10K docs):** ~Same as before (0.5-2 sec)
- **Medium DBs (100K docs):** ~10-20% slower (batch overhead)
- **Large DBs (1M+ docs):** **10x faster** (no memory thrashing)

**Memory:**
- **Before:** O(n) = 1GB @ 1M docs
- **After:** O(1) = 32MB fixed

**Disk I/O:**
- Sequential writes (same as before)
- +1 fsync per batch (negligible overhead)

---

### 1.3 Error Context Enhancement

#### Problem Statement

**Current Error Messages:**
```rust
Error: CollectionNotFound("users")
```

**Better Error Messages:**
```rust
Error: CollectionNotFound("users")

Caused by:
    0: Failed to query collection 'users' with filter {"age": {"$gte": 18}}
    1: Collection does not exist in database 'myapp.mlite'
    2: Available collections: ["posts", "comments"]
```

#### Solution: Use `anyhow::Context`

**Implementation:**

```rust
// Already in Cargo.toml dependencies!
// anyhow = "1.0"

// ironbase-core/src/collection_core.rs

use anyhow::Context;

impl CollectionCore {
    pub fn find(&self, query_json: &Value) -> Result<Vec<Value>> {
        let parsed_query = Query::from_json(query_json)
            .context(format!("Failed to parse query: {:?}", query_json))?;

        let storage = self.storage.read();
        let meta = storage.get_collection_meta(&self.name)
            .context(format!("Collection '{}' does not exist", self.name))?;

        // ... rest of implementation ...
    }

    pub fn insert_one(&self, mut fields: HashMap<String, Value>) -> Result<DocumentId> {
        let mut storage = self.storage.write();

        let meta = storage.get_collection_meta_mut(&self.name)
            .context(format!("Failed to insert into collection '{}'", self.name))?;

        // ... rest of implementation ...
    }
}
```

#### Implementation Steps

1. **30 minutes:** Add `use anyhow::Context` to relevant modules
2. **1-2 hours:** Add `.context()` to all major error paths (find, insert, update, delete)
3. **30 minutes:** Test error messages in integration tests

#### Testing

```rust
#[test]
fn test_error_context_collection_not_found() {
    let db = IronBase::open("test.mlite").unwrap();

    let result = db.collection("nonexistent").find(&json!({}));

    assert!(result.is_err());
    let err = result.unwrap_err();
    let err_msg = format!("{:#}", err);  // Pretty-print with backtrace

    assert!(err_msg.contains("CollectionNotFound"));
    assert!(err_msg.contains("nonexistent"));
}
```

#### Breaking Changes

**None** - Error types remain the same, just better messages.

---

## Phase 2: Performance Improvements

**Goal:** Maximize concurrent throughput and reduce latency
**Duration:** 2-3 weeks
**Priority:** âš¡ MEDIUM

### 2.1 MVCC (Multi-Version Concurrency Control)

#### Problem Statement

**Current Bottleneck:**
```rust
let mut storage = self.storage.write();  // Blocks ALL reads!
```

**Scenario:**
1. Writer acquires write lock (blocks ALL readers)
2. Writer takes 100ms to complete
3. 100 concurrent readers wait 100ms each
4. Total throughput: 10 ops/sec

**After MVCC:**
1. Writer creates new version (no lock on old version)
2. Readers continue reading old version (snapshot isolation)
3. Writer commits new version atomically
4. Total throughput: 1000+ ops/sec

#### Solution Design: MVCC with Snapshot Isolation

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MVCC Storage Architecture                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Version Timeline:                                        â”‚
â”‚                                                            â”‚
â”‚  Version 1 (t=0)    Version 2 (t=10)   Version 3 (t=20)  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ {age:25} â”‚  â”€â”€â”€â–¶ â”‚ {age:26} â”‚  â”€â”€â”€â–¶ â”‚ {age:27} â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚       â–²                   â–²                   â–²           â”‚
â”‚       â”‚                   â”‚                   â”‚           â”‚
â”‚  Reader 1            Reader 2            Writer           â”‚
â”‚  (snapshot@v1)       (snapshot@v2)       (creates v3)    â”‚
â”‚                                                            â”‚
â”‚  â–º Readers never block each other                         â”‚
â”‚  â–º Readers never block writers                            â”‚
â”‚  â–º Writers commit atomically (CAS on version counter)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Concepts:**
- **Version Counter:** Atomically incremented on each write
- **Snapshot:** Reader gets consistent view at a specific version
- **Garbage Collection:** Old versions cleaned up when no readers reference them

#### Pseudocode Implementation

**Step 1: Define Version Structures**

```rust
// ironbase-core/src/storage/mvcc.rs

use std::sync::atomic::{AtomicU64, Ordering};
use dashmap::DashMap;  // Lock-free concurrent HashMap

/// Versioned document wrapper
#[derive(Debug, Clone)]
pub struct VersionedDocument {
    pub doc_id: DocumentId,
    pub version: u64,
    pub data: Value,
    pub offset: u64,  // File offset for this version
    pub deleted: bool,  // Tombstone flag
}

/// Version metadata
#[derive(Debug, Clone)]
pub struct VersionMetadata {
    pub version: u64,
    pub timestamp: u64,
    pub committed: bool,
}

/// MVCC Storage Engine
pub struct MVCCStorageEngine {
    file: File,
    file_path: String,

    /// Current version (atomically incremented)
    current_version: AtomicU64,

    /// Version index: DocumentId â†’ Vec<VersionedDocument>
    /// Sorted by version descending (newest first)
    version_index: DashMap<DocumentId, Vec<VersionedDocument>>,

    /// Active snapshots: (version, reader_count)
    /// Used for garbage collection
    active_snapshots: DashMap<u64, usize>,

    /// Committed versions metadata
    committed_versions: DashMap<u64, VersionMetadata>,
}

impl MVCCStorageEngine {
    pub fn open(path: &str) -> Result<Self> {
        let file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .open(path)?;

        let mut engine = MVCCStorageEngine {
            file,
            file_path: path.to_string(),
            current_version: AtomicU64::new(1),
            version_index: DashMap::new(),
            active_snapshots: DashMap::new(),
            committed_versions: DashMap::new(),
        };

        // Load version index from file
        engine.load_version_index()?;

        Ok(engine)
    }
}
```

**Step 2: Implement Snapshot Reads**

```rust
// ironbase-core/src/storage/mvcc.rs

/// Read transaction with snapshot isolation
pub struct ReadTransaction<'a> {
    storage: &'a MVCCStorageEngine,
    snapshot_version: u64,
}

impl<'a> ReadTransaction<'a> {
    /// Get document at snapshot version
    pub fn read_document(&self, doc_id: &DocumentId) -> Option<Value> {
        if let Some(versions) = self.storage.version_index.get(doc_id) {
            // Find latest version <= snapshot_version
            for versioned_doc in versions.value() {
                if versioned_doc.version <= self.snapshot_version && !versioned_doc.deleted {
                    return Some(versioned_doc.data.clone());
                }
            }
        }

        None
    }

    /// Scan collection at snapshot version
    pub fn scan_collection(&self, collection_name: &str) -> Vec<Value> {
        let mut results = Vec::new();

        for entry in self.storage.version_index.iter() {
            let doc_id = entry.key();
            let versions = entry.value();

            // Find latest version <= snapshot_version
            for versioned_doc in versions {
                if versioned_doc.version <= self.snapshot_version && !versioned_doc.deleted {
                    // Check if document belongs to this collection
                    if versioned_doc.data.get("_collection")
                        .and_then(|v| v.as_str()) == Some(collection_name)
                    {
                        results.push(versioned_doc.data.clone());
                    }
                    break;  // Found latest version, stop searching
                }
            }
        }

        results
    }
}

impl<'a> Drop for ReadTransaction<'a> {
    fn drop(&mut self) {
        // Decrement reader count for this snapshot
        if let Some(mut count) = self.storage.active_snapshots.get_mut(&self.snapshot_version) {
            *count -= 1;
            if *count == 0 {
                // Last reader of this snapshot, can garbage collect
                self.storage.active_snapshots.remove(&self.snapshot_version);
            }
        }
    }
}

impl MVCCStorageEngine {
    /// Begin read transaction (snapshot isolation)
    pub fn begin_read(&self) -> ReadTransaction {
        let snapshot_version = self.current_version.load(Ordering::SeqCst);

        // Increment reader count for this snapshot
        self.active_snapshots.entry(snapshot_version)
            .and_modify(|count| *count += 1)
            .or_insert(1);

        ReadTransaction {
            storage: self,
            snapshot_version,
        }
    }
}
```

**Step 3: Implement Write Transactions**

```rust
// ironbase-core/src/storage/mvcc.rs

/// Write transaction with staging area
pub struct WriteTransaction {
    version: u64,
    staging: Vec<(DocumentId, Value, bool)>,  // (doc_id, data, deleted)
}

impl WriteTransaction {
    /// Stage document write
    pub fn write(&mut self, doc_id: DocumentId, data: Value) {
        self.staging.push((doc_id, data, false));
    }

    /// Stage document delete
    pub fn delete(&mut self, doc_id: DocumentId) {
        self.staging.push((doc_id, Value::Null, true));
    }
}

impl MVCCStorageEngine {
    /// Begin write transaction
    pub fn begin_write(&self) -> WriteTransaction {
        let version = self.current_version.fetch_add(1, Ordering::SeqCst) + 1;

        WriteTransaction {
            version,
            staging: Vec::new(),
        }
    }

    /// Commit write transaction
    pub fn commit_write(&mut self, tx: WriteTransaction) -> Result<()> {
        // Write all staged changes to file
        for (doc_id, data, deleted) in tx.staging {
            let offset = if !deleted {
                // Write document data to file
                let doc_bytes = serde_json::to_vec(&data)?;
                self.file.seek(SeekFrom::End(0))?;
                let offset = self.file.stream_position()?;
                self.file.write_all(&(doc_bytes.len() as u32).to_le_bytes())?;
                self.file.write_all(&doc_bytes)?;
                offset
            } else {
                0  // Tombstone has no data
            };

            // Add new version to version index
            let versioned_doc = VersionedDocument {
                doc_id: doc_id.clone(),
                version: tx.version,
                data,
                offset,
                deleted,
            };

            self.version_index.entry(doc_id)
                .and_modify(|versions| {
                    // Insert at front (newest first)
                    versions.insert(0, versioned_doc.clone());
                })
                .or_insert_with(|| vec![versioned_doc]);
        }

        // Fsync file
        self.file.sync_all()?;

        // Mark version as committed
        self.committed_versions.insert(tx.version, VersionMetadata {
            version: tx.version,
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            committed: true,
        });

        Ok(())
    }
}
```

**Step 4: Garbage Collection**

```rust
// ironbase-core/src/storage/mvcc.rs

impl MVCCStorageEngine {
    /// Garbage collect old versions no longer visible to any snapshot
    pub fn garbage_collect(&mut self) -> Result<u64> {
        let min_active_snapshot = self.active_snapshots.iter()
            .map(|entry| *entry.key())
            .min()
            .unwrap_or(u64::MAX);

        let mut versions_removed = 0u64;

        for mut entry in self.version_index.iter_mut() {
            let versions = entry.value_mut();

            // Keep only:
            // 1. Latest version (always needed)
            // 2. Versions >= min_active_snapshot
            let mut new_versions = Vec::new();

            if let Some(latest) = versions.first() {
                new_versions.push(latest.clone());  // Always keep latest
            }

            for version in versions.iter().skip(1) {
                if version.version >= min_active_snapshot {
                    new_versions.push(version.clone());
                }
            }

            versions_removed += (versions.len() - new_versions.len()) as u64;
            *versions = new_versions;
        }

        Ok(versions_removed)
    }

    /// Background garbage collection thread
    pub fn start_gc_thread(engine: Arc<Mutex<MVCCStorageEngine>>) {
        thread::spawn(move || {
            loop {
                thread::sleep(Duration::from_secs(60));  // GC every minute

                if let Ok(mut engine) = engine.lock() {
                    match engine.garbage_collect() {
                        Ok(removed) => {
                            if removed > 0 {
                                println!("GC: Removed {} old versions", removed);
                            }
                        }
                        Err(e) => eprintln!("GC error: {}", e),
                    }
                }
            }
        });
    }
}
```

**Step 5: Integration with CollectionCore**

```rust
// ironbase-core/src/collection_core.rs

impl CollectionCore {
    /// Find with MVCC (no lock needed!)
    pub fn find(&self, query_json: &Value) -> Result<Vec<Value>> {
        let parsed_query = Query::from_json(query_json)?;

        // Begin read transaction (snapshot)
        let tx = self.storage.begin_read();

        // Scan collection at snapshot version
        let all_docs = tx.scan_collection(&self.name);

        // Filter by query
        let mut results = Vec::new();
        for doc in all_docs {
            if parsed_query.matches(&doc) {
                results.push(doc);
            }
        }

        Ok(results)
        // tx drops here, decrementing snapshot reader count
    }

    /// Insert with MVCC
    pub fn insert_one(&self, fields: HashMap<String, Value>) -> Result<DocumentId> {
        // Begin write transaction
        let mut tx = self.storage.begin_write();

        // Generate document ID
        let doc_id = DocumentId::new_auto(/* ... */);

        // Stage write
        tx.write(doc_id.clone(), json!(fields));

        // Commit transaction
        self.storage.commit_write(tx)?;

        Ok(doc_id)
    }
}
```

#### Implementation Steps

1. **Day 1-3:** Implement `MVCCStorageEngine` core structures
2. **Day 4-5:** Implement `ReadTransaction` with snapshot isolation
3. **Day 6-7:** Implement `WriteTransaction` with commit logic
4. **Day 8-9:** Implement garbage collection
5. **Day 10-12:** Integrate with `CollectionCore`
6. **Day 13-14:** Migration from old storage format

#### Testing Strategy

```rust
#[test]
fn test_mvcc_snapshot_isolation() {
    let storage = Arc::new(Mutex::new(MVCCStorageEngine::open("test.mlite").unwrap()));

    // Writer thread
    let storage_clone = Arc::clone(&storage);
    let writer = thread::spawn(move || {
        let mut tx = storage_clone.lock().unwrap().begin_write();
        tx.write(DocumentId::Int(1), json!({"value": 100}));
        thread::sleep(Duration::from_millis(100));  // Simulate slow write
        storage_clone.lock().unwrap().commit_write(tx).unwrap();
    });

    // Reader thread (starts before write completes)
    thread::sleep(Duration::from_millis(50));
    let storage_clone = Arc::clone(&storage);
    let reader = thread::spawn(move || {
        let tx = storage_clone.lock().unwrap().begin_read();
        let doc = tx.read_document(&DocumentId::Int(1));
        assert!(doc.is_none());  // Should NOT see uncommitted write!
    });

    writer.join().unwrap();
    reader.join().unwrap();
}

#[test]
fn test_mvcc_concurrent_reads() {
    let storage = Arc::new(Mutex::new(MVCCStorageEngine::open("test.mlite").unwrap()));

    // Insert initial document
    {
        let mut tx = storage.lock().unwrap().begin_write();
        tx.write(DocumentId::Int(1), json!({"value": 42}));
        storage.lock().unwrap().commit_write(tx).unwrap();
    }

    // Spawn 100 concurrent readers
    let mut handles = vec![];
    for _ in 0..100 {
        let storage_clone = Arc::clone(&storage);
        let handle = thread::spawn(move || {
            let tx = storage_clone.lock().unwrap().begin_read();
            let doc = tx.read_document(&DocumentId::Int(1));
            assert_eq!(doc.unwrap().get("value").unwrap(), &json!(42));
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }
}

#[test]
fn test_mvcc_garbage_collection() {
    let mut storage = MVCCStorageEngine::open("test.mlite").unwrap();

    // Create 10 versions of same document
    for i in 0..10 {
        let mut tx = storage.begin_write();
        tx.write(DocumentId::Int(1), json!({"version": i}));
        storage.commit_write(tx).unwrap();
    }

    // All 10 versions should exist
    let versions = storage.version_index.get(&DocumentId::Int(1)).unwrap();
    assert_eq!(versions.len(), 10);

    // No active snapshots â†’ garbage collect
    let removed = storage.garbage_collect().unwrap();
    assert_eq!(removed, 9);  // Keep only latest version

    let versions = storage.version_index.get(&DocumentId::Int(1)).unwrap();
    assert_eq!(versions.len(), 1);
}
```

#### Breaking Changes

**YES - Storage Format Change**

**Migration Required:**
```rust
// Migration tool
fn migrate_to_mvcc(old_db_path: &str, new_db_path: &str) -> Result<()> {
    let old_db = StorageEngine::open(old_db_path)?;
    let mut new_db = MVCCStorageEngine::open(new_db_path)?;

    // Copy all documents at version 1
    for (collection_name, meta) in old_db.collections() {
        let docs = old_db.scan_collection(collection_name)?;

        let mut tx = new_db.begin_write();
        for doc in docs {
            if let Some(id_val) = doc.get("_id") {
                let doc_id = serde_json::from_value::<DocumentId>(id_val.clone())?;
                tx.write(doc_id, doc);
            }
        }
        new_db.commit_write(tx)?;
    }

    Ok(())
}
```

#### Performance Impact

**Throughput:**
- **Before:** 100-1000 concurrent reads/sec (write lock contention)
- **After:** **10,000-100,000 concurrent reads/sec** (no contention!)

**Latency:**
- Read latency: -50% (no lock wait)
- Write latency: +10% (version tracking overhead)

**Memory:**
- Overhead: ~32 bytes per version (version metadata)
- Typical: 2-3 versions per document = 64-96 bytes/doc
- GC keeps memory bounded

---

### 2.2 Query Plan Caching

**(Abbreviated - see similar pattern as LRU query cache)**

**Implementation:**
```rust
pub struct QueryPlanCache {
    cache: LruCache<QueryHash, QueryPlan>,
}

impl CollectionCore {
    pub fn find_with_plan_cache(&self, query_json: &Value) -> Result<Vec<Value>> {
        let query_hash = QueryHash::new(&self.name, query_json);

        let plan = if let Some(cached_plan) = self.plan_cache.get(&query_hash) {
            cached_plan
        } else {
            let plan = QueryPlanner::analyze_query(query_json, &self.indexes.list())?;
            self.plan_cache.insert(query_hash, plan.clone());
            plan
        };

        self.execute_plan(plan)
    }
}
```

**Impact:** 10-50x speedup for repeated query patterns

---

### 2.3 Persistent Catalog

**(Abbreviated)**

**Implementation:**
```rust
impl StorageEngine {
    pub fn save_catalog(&self, collection_name: &str) -> Result<()> {
        let catalog_path = format!("{}.catalog", collection_name);
        let meta = self.get_collection_meta(collection_name)?;
        let catalog_bytes = bincode::serialize(&meta.document_catalog)?;
        fs::write(catalog_path, catalog_bytes)?;
        Ok(())
    }

    pub fn load_catalog(&mut self, collection_name: &str) -> Result<()> {
        let catalog_path = format!("{}.catalog", collection_name);
        if Path::new(&catalog_path).exists() {
            let catalog_bytes = fs::read(catalog_path)?;
            let catalog: HashMap<DocumentId, u64> = bincode::deserialize(&catalog_bytes)?;

            let meta = self.get_collection_meta_mut(collection_name)?;
            meta.document_catalog = catalog;
        }
        Ok(())
    }
}
```

**Impact:** Instant startup (no rebuild scan), scalable to 100M+ documents

---

## Phase 3: Feature Completeness

**Goal:** MongoDB near-parity feature support
**Duration:** 3-4 weeks
**Priority:** ğŸ’¡ MEDIUM

### 3.1 Compound Index Support

**(Full implementation in separate section due to length)**

**Key Concepts:**
- Multi-field indexes: `(city, age)`
- Composite keys in B+ tree
- Query optimizer updates for compound index selection

**Example:**
```rust
// Create compound index
db.collection("users").create_compound_index(&["city", "age"], false)?;

// Query uses compound index
db.collection("users").find(&json!({
    "city": "NYC",
    "age": {"$gte": 18}
}))  // â†’ O(log n) with compound index!
```

---

### 3.2 Aggregation Pipeline Extensions

**New Stages:**

#### `$lookup` (Join Operation)

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LookupStage {
    pub from: String,           // Foreign collection
    pub local_field: String,    // Field in current collection
    pub foreign_field: String,  // Field in foreign collection
    pub as_field: String,       // Output array field
}

impl LookupStage {
    pub fn execute(&self, docs: Vec<Value>, storage: &StorageEngine) -> Result<Vec<Value>> {
        let mut results = Vec::new();

        for mut doc in docs {
            let local_value = doc.get(&self.local_field).cloned().unwrap_or(Value::Null);

            // Find matching documents in foreign collection
            let foreign_docs = storage.scan_collection(&self.from)?
                .into_iter()
                .filter(|foreign_doc| {
                    foreign_doc.get(&self.foreign_field) == Some(&local_value)
                })
                .collect::<Vec<_>>();

            // Add as array field
            doc.as_object_mut().unwrap().insert(
                self.as_field.clone(),
                json!(foreign_docs)
            );

            results.push(doc);
        }

        Ok(results)
    }
}
```

**Usage:**
```rust
db.collection("orders").aggregate(&json!([
    {
        "$lookup": {
            "from": "users",
            "localField": "user_id",
            "foreignField": "_id",
            "as": "user_info"
        }
    }
]))
```

#### `$unwind` (Array Deconstruction)

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UnwindStage {
    pub path: String,  // Field path to array
    pub preserve_null_and_empty_arrays: bool,
}

impl UnwindStage {
    pub fn execute(&self, docs: Vec<Value>) -> Result<Vec<Value>> {
        let mut results = Vec::new();

        for doc in docs {
            if let Some(array_val) = doc.get(&self.path) {
                if let Some(array) = array_val.as_array() {
                    if array.is_empty() && self.preserve_null_and_empty_arrays {
                        results.push(doc.clone());
                    } else {
                        // Create one document per array element
                        for element in array {
                            let mut new_doc = doc.clone();
                            new_doc.as_object_mut().unwrap().insert(
                                self.path.clone(),
                                element.clone()
                            );
                            results.push(new_doc);
                        }
                    }
                } else if self.preserve_null_and_empty_arrays {
                    results.push(doc.clone());
                }
            } else if self.preserve_null_and_empty_arrays {
                results.push(doc.clone());
            }
        }

        Ok(results)
    }
}
```

**Usage:**
```rust
db.collection("users").aggregate(&json!([
    {"$unwind": {"path": "hobbies", "preserveNullAndEmptyArrays": false}}
]))

// Input:  {name: "Alice", hobbies: ["reading", "coding"]}
// Output: {name: "Alice", hobbies: "reading"}
//         {name: "Alice", hobbies: "coding"}
```

---

### 3.3 Text Search Index

**Inverted Index Implementation:**

```rust
pub struct TextIndex {
    /// term â†’ [DocumentId]
    terms: HashMap<String, Vec<DocumentId>>,

    /// Document ID â†’ term frequency
    doc_term_freq: HashMap<DocumentId, HashMap<String, usize>>,

    /// Text analyzer (tokenizer + stemmer)
    analyzer: TextAnalyzer,
}

pub struct TextAnalyzer {
    stop_words: HashSet<String>,
}

impl TextAnalyzer {
    pub fn tokenize(&self, text: &str) -> Vec<String> {
        text.to_lowercase()
            .split_whitespace()
            .filter(|word| !self.stop_words.contains(*word))
            .map(|word| self.stem(word))  // Porter stemming
            .collect()
    }

    fn stem(&self, word: &str) -> String {
        // Simple stemming (production: use rust-stem crate)
        word.trim_end_matches("ing")
            .trim_end_matches("ed")
            .trim_end_matches("s")
            .to_string()
    }
}

impl TextIndex {
    pub fn insert(&mut self, doc_id: DocumentId, text: &str) {
        let tokens = self.analyzer.tokenize(text);
        let mut term_freq = HashMap::new();

        for token in tokens {
            // Update inverted index
            self.terms.entry(token.clone())
                .or_insert_with(Vec::new)
                .push(doc_id.clone());

            // Update term frequency
            *term_freq.entry(token).or_insert(0) += 1;
        }

        self.doc_term_freq.insert(doc_id, term_freq);
    }

    pub fn search(&self, query: &str) -> Vec<DocumentId> {
        let query_tokens = self.analyzer.tokenize(query);

        if query_tokens.is_empty() {
            return vec![];
        }

        // Start with documents containing first term
        let mut result = self.terms.get(&query_tokens[0])
            .cloned()
            .unwrap_or_default();

        // Intersect with documents containing other terms
        for token in &query_tokens[1..] {
            if let Some(term_docs) = self.terms.get(token) {
                result.retain(|id| term_docs.contains(id));
            } else {
                return vec![];  // Term not found â†’ no results
            }
        }

        result
    }

    pub fn search_with_scoring(&self, query: &str) -> Vec<(DocumentId, f64)> {
        let query_tokens = self.analyzer.tokenize(query);
        let mut scores: HashMap<DocumentId, f64> = HashMap::new();

        // TF-IDF scoring
        for token in query_tokens {
            if let Some(term_docs) = self.terms.get(&token) {
                let idf = (self.doc_term_freq.len() as f64 / term_docs.len() as f64).ln();

                for doc_id in term_docs {
                    if let Some(term_freq) = self.doc_term_freq.get(doc_id)
                        .and_then(|freqs| freqs.get(&token))
                    {
                        let tf = *term_freq as f64;
                        *scores.entry(doc_id.clone()).or_insert(0.0) += tf * idf;
                    }
                }
            }
        }

        // Sort by score descending
        let mut results: Vec<(DocumentId, f64)> = scores.into_iter().collect();
        results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

        results
    }
}
```

**Usage:**
```rust
// Create text index
db.collection("posts").create_text_index("content")?;

// Search
let results = db.collection("posts").text_search("rust database performance")?;
```

---

## Phase 4: Scalability

**Goal:** Support 10M+ documents, 1000+ concurrent users
**Duration:** 4-6 weeks
**Priority:** ğŸš€ LOW

### 4.1 Multi-File Architecture

**Design:**
```
database/
â”œâ”€â”€ metadata.mlite        # Database-wide metadata
â”œâ”€â”€ users.mlite          # Collection data files
â”œâ”€â”€ posts.mlite
â”œâ”€â”€ comments.mlite
â”œâ”€â”€ users_age.idx        # Index files (separate)
â”œâ”€â”€ users_email.idx
â””â”€â”€ database.wal         # Shared WAL
```

**Benefits:**
- Collection-level isolation (parallel I/O)
- Independent compaction per collection
- Easier backup (file-level granularity)

---

### 4.2 In-Place Update Optimization

**Selective In-Place Writes:**

```rust
impl StorageEngine {
    pub fn can_update_in_place(&self, offset: u64, old_doc: &Value, new_doc: &Value) -> bool {
        // Only if:
        // 1. Document size doesn't change
        // 2. Field is atomic (no nested updates)
        // 3. Not a transaction (would break WAL replay)
        // 4. Document is not indexed (would need index update)
        // 5. File is not mmap'd at this offset (would need remapping)

        let old_size = serde_json::to_vec(old_doc).unwrap().len();
        let new_size = serde_json::to_vec(new_doc).unwrap().len();

        old_size == new_size
    }

    pub fn update_in_place(&mut self, offset: u64, new_doc: &Value) -> Result<()> {
        let doc_bytes = serde_json::to_vec(new_doc)?;

        self.file.seek(SeekFrom::Start(offset + 4))?;  // Skip length prefix
        self.file.write_all(&doc_bytes)?;
        self.file.sync_data()?;  // Sync only data, not metadata

        Ok(())
    }
}
```

**Trade-off:**
- **Pros:** 6x â†’ 2-3x write amplification reduction
- **Cons:** Komplexebb recovery logic, crash safety checks

---

### 4.3 Background Compaction

**Online Compaction:**

```rust
pub struct BackgroundCompactor {
    storage: Arc<Mutex<StorageEngine>>,
    running: Arc<AtomicBool>,
    progress: Arc<Mutex<CompactionProgress>>,
}

pub struct CompactionProgress {
    pub phase: CompactionPhase,
    pub documents_processed: u64,
    pub total_documents: u64,
}

impl BackgroundCompactor {
    pub fn start(storage: Arc<Mutex<StorageEngine>>) -> Self {
        let running = Arc::new(AtomicBool::new(true));
        let progress = Arc::new(Mutex::new(CompactionProgress::default()));

        let compactor = BackgroundCompactor {
            storage: Arc::clone(&storage),
            running: Arc::clone(&running),
            progress: Arc::clone(&progress),
        };

        // Spawn background thread
        let storage_clone = Arc::clone(&storage);
        let progress_clone = Arc::clone(&progress);
        thread::spawn(move || {
            while running.load(Ordering::Relaxed) {
                // Compact every hour
                thread::sleep(Duration::from_secs(3600));

                // Run compaction
                if let Ok(mut storage) = storage_clone.lock() {
                    match storage.compact_streaming() {
                        Ok(stats) => {
                            println!("Background compaction completed: {:?}", stats);
                        }
                        Err(e) => {
                            eprintln!("Background compaction failed: {}", e);
                        }
                    }
                }
            }
        });

        compactor
    }

    pub fn stop(&self) {
        self.running.store(false, Ordering::Relaxed);
    }

    pub fn progress(&self) -> CompactionProgress {
        self.progress.lock().unwrap().clone()
    }
}
```

---

## Testing Strategy

### Unit Tests
- Every new function has dedicated unit test
- Edge cases explicitly tested (empty, null, overflow, etc.)

### Integration Tests
- End-to-end workflows (insert â†’ query â†’ update â†’ delete)
- Multi-collection scenarios
- Transaction rollback/commit

### Performance Benchmarks
- Before/after comparison for every optimization
- Regression detection (CI/CD integration)

### Crash Recovery Tests
```rust
#[test]
fn test_crash_during_index_update() {
    // 1. Start transaction
    // 2. Write to WAL
    // 3. PANIC! (simulate crash)
    // 4. Reopen database
    // 5. Verify data consistency
}
```

### Concurrent Access Tests
```rust
#[test]
fn test_1000_concurrent_readers() {
    // Spawn 1000 threads, all reading simultaneously
    // Measure throughput and latency
}
```

---

## Migration Guide

### Phase 1 â†’ No Migration Required
- Index atomicity is backward compatible
- Streaming compaction is drop-in replacement

### Phase 2 â†’ MVCC Migration Required

**Step 1: Backup**
```bash
cp myapp.mlite myapp.mlite.backup
```

**Step 2: Run Migration Tool**
```bash
ironbase-migrate --input myapp.mlite --output myapp_mvcc.mlite --format mvcc
```

**Step 3: Verify**
```bash
ironbase-verify myapp_mvcc.mlite
```

**Step 4: Switch Application**
```rust
// Old:
let db = IronBase::open("myapp.mlite")?;

// New:
let db = IronBase::open_mvcc("myapp_mvcc.mlite")?;
```

---

## Appendix: Trade-offs & Alternatives

### MVCC vs Lock-Free Structures

| Approach | Pros | Cons |
|----------|------|------|
| **MVCC** | - Snapshot isolation<br>- Simple API<br>- Production-proven | - GC overhead<br>- Memory for versions |
| **Lock-Free** | - No GC needed<br>- Lower latency | - Complex implementation<br>- ABA problem |
| **Verdict** | âœ… **MVCC** (better trade-off) |  |

### Streaming vs Incremental Compaction

| Approach | Pros | Cons |
|----------|------|------|
| **Streaming** | - Constant memory<br>- Simple | - Still blocks operations |
| **Incremental** | - Online (no blocking) | - Complex state management |
| **Verdict** | âœ… **Streaming first**, Incremental later |  |

---

## Summary

This implementation guide provides **execution-ready pseudocode** for all 4 phases of IronBase improvements. Each section includes:

- âœ… Problem statement with concrete examples
- âœ… Solution design with architecture diagrams
- âœ… Step-by-step pseudocode (150+ lines per major feature)
- âœ… Implementation timeline (days/weeks)
- âœ… Comprehensive testing strategy
- âœ… Breaking changes and migration paths
- âœ… Performance impact analysis

**Total Implementation Time:** 8-16 weeks

**Recommended Order:**
1. **Phase 1 (CRITICAL)** - Index atomicity + Streaming compaction
2. **Phase 2 (HIGH)** - MVCC for concurrency
3. **Phase 3 (MEDIUM)** - Feature completeness
4. **Phase 4 (LOW)** - Long-term scalability

---

**Document Version:** 1.0
**Last Updated:** 2025-11-11
**Author:** Engineering Code Analysis Report + Claude Code
